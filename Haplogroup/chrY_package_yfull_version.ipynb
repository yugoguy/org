{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6bf85823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(vcf_path, update=False, vcf_df_formatted_path=None):\n",
    "    #imports and classification dataframe preparation\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    %matplotlib inline\n",
    "    import networkx as nx\n",
    "    import json\n",
    "    import requests\n",
    "    \n",
    "    print(\"Credit: YFull\")\n",
    "    print(\"https://www.yfull.com/tree/\")\n",
    "    print(\"https://github.com/YFullTeam/YTree\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Reading VCF file...\")\n",
    "    # vcf file preparation\n",
    "    if vcf_df_formatted_path:\n",
    "        print(\"reads pickle saved dataframe with GRCh38 position, reference allele, and variant allele list as columns\")\n",
    "        var = pd.read_pickle(vcf_df_formatted_path)\n",
    "        var.columns = [[\"GRCh38pos\" ,\"ref\", \"var\"]]\n",
    "    else:  \n",
    "        vcf = pd.read_table(vcf_path, header=None)\n",
    "        vcfy = vcf[vcf[0]==\"chrY\"]\n",
    "        vcfy = vcfy.reset_index(drop=True)\n",
    "        var_info = []\n",
    "        for i in vcfy.index:\n",
    "            var_info.append(vcfy.iloc[i][9].split(\":\")[0])\n",
    "        vcfy[\"var_info\"] = var_info\n",
    "        vars_ = []\n",
    "        for i in vcfy.index:\n",
    "            var = []\n",
    "            var_info = vcfy.iloc[i][\"var_info\"]\n",
    "            alt_which = var_info.split(\"/\")\n",
    "            alt_choice = vcfy.iloc[i][4].split(\",\")\n",
    "            for j in alt_which:\n",
    "                if int(j)!=0:\n",
    "                    if alt_choice[int(j)-1] not in var:\n",
    "                        var.append(alt_choice[int(j)-1])\n",
    "            vars_.append(var)\n",
    "        vcfy[\"vars\"] = vars_\n",
    "        var = vcfy[[1,3,\"vars\"]]\n",
    "        var.columns = [[\"GRCh38pos\" ,\"ref\", \"var\"]]\n",
    "    \n",
    "    # analysis snp database preparation \n",
    "    tree = pd.read_pickle(open(\"yfull_tree_info.sav\", 'rb'))\n",
    "    snps = pd.read_pickle(open(\"full_snps.sav\", 'rb'))\n",
    "    \n",
    "    # update tree choice\n",
    "    if update:\n",
    "        update = input(\"update the information used in the analysis? this may take a while [y/n]\")\n",
    "    \n",
    "    if update==\"y\":\n",
    "        def recur_tree(tree=None, init=True):\n",
    "            if init:\n",
    "                tree = None\n",
    "                response = requests.get(\"https://raw.githubusercontent.com/YFullTeam/YTree/master/current_tree.json\")\n",
    "                tree = pd.DataFrame(json.loads(response.content))\n",
    "                tree[\"parent\"] = \"root\"\n",
    "                tree[\"id\"] = \"Y Chromosomal Adam\"\n",
    "            children = []\n",
    "            child_dicts = []\n",
    "            if \"children\" in tree.columns:\n",
    "                for i in range(len(tree)):\n",
    "                    child_dict = tree.iloc[i][\"children\"]\n",
    "                    child_dicts.append(child_dict)\n",
    "                hash_string = \"\"\n",
    "                for child_dict in child_dicts:\n",
    "                    hash_ = hash_string + json.dumps(child_dict)\n",
    "                hash_ = hash(hash_string)\n",
    "                for child_dict in child_dicts:\n",
    "                    index_ = [0] if \"children\" not in child_dict.keys() else None\n",
    "                    child_df = pd.DataFrame(child_dict, index=index_)\n",
    "                    child_df[\"parent\"] = hash_\n",
    "                    children.append(child_df)\n",
    "                for child in children:\n",
    "                    tree = pd.concat([tree, recur_tree(tree=child, init=False)])\n",
    "            return tree\n",
    "\n",
    "        def clean_tree(tree):\n",
    "            tree = tree.reset_index(drop=True).fillna(\"leaf\")\n",
    "\n",
    "            listed_snps = []\n",
    "            for i in tree.index:\n",
    "                snps = re.split('/|, ', tree.iloc[i][\"snps\"])\n",
    "                listed_snps.append(snps)\n",
    "            tree[\"snp_list\"] = listed_snps\n",
    "\n",
    "            counts = []\n",
    "            scores = []\n",
    "            for i in tree.index:\n",
    "                snp_list = tree.iloc[i][\"snps\"].split(\",\")\n",
    "                count = len(snp_list)\n",
    "                if snp_list == ['']:\n",
    "                    count = 0\n",
    "                counts.append(count)\n",
    "                if count!=0:\n",
    "                    score = 1/count\n",
    "                else:\n",
    "                    score = 0\n",
    "                scores.append(score)\n",
    "            tree[\"count\"] = counts\n",
    "            tree[\"score\"] = scores\n",
    "\n",
    "            childs = tree.groupby(\"id\")[\"children\"].apply(list)\n",
    "\n",
    "            hashes = []\n",
    "            for i in tree.index:\n",
    "                hash_ = hash(json.dumps(tree.iloc[i][\"children\"]))\n",
    "                hashes.append(hash_)\n",
    "            tree[\"hash\"] = hashes\n",
    "\n",
    "            return tree\n",
    "        \n",
    "        if update==\"y\":\n",
    "            tree = clean_tree(recur_tree())\n",
    "\n",
    "        # update ybrowse master file choice\n",
    "\n",
    "        snps_list_slashed = []\n",
    "        for i in tree.index:\n",
    "            snps = tree.iloc[i][\"snps\"]\n",
    "            snps_split = snps.split(\",\")\n",
    "            for snp in snps_split:\n",
    "                snps_list_slashed.append(snp)\n",
    "\n",
    "        snps_list_slash_firsts = []\n",
    "        for snp in snps_list_slashed:\n",
    "            un_spaced = snp.replace(\" \", \"\")\n",
    "            slashed = un_spaced.split(\"/\")\n",
    "            selected = un_spaced.split(\"/\")[0]\n",
    "            snps_list_slash_firsts.append(selected)\n",
    "\n",
    "        snps_list_slash_firsts = list(filter(None, snps_list_slash_firsts))\n",
    "        snps_unique = list(set(snps_list_slash_firsts))\n",
    "        if update==\"y\":\n",
    "            print(\"updating ISOGG Ybrowse SNP index. it may take several minutes to update!\")\n",
    "            isyb = pd.read_csv(\"https://ybrowse.org/gbrowse2/gff/snps_hg38.csv\")\n",
    "            isyb.to_csv(\"snps_hg38.csv\")\n",
    "\n",
    "        # update yfull snp index choice\n",
    "        if update==\"y\":\n",
    "            print(\"updating YFull SNP index. it may take several minutes to update!\")\n",
    "            htmlraw = requests.get(f\"https://www.yfull.com/snp-list/?page={1}\")\n",
    "            content = pd.read_html(htmlraw.content, header=0)\n",
    "            YFull = content[1]\n",
    "            page = 2\n",
    "            while True:\n",
    "                try: \n",
    "                    htmlraw = requests.get(f\"https://www.yfull.com/snp-list/?page={page}\")\n",
    "                    content = pd.read_html(htmlraw.content, header=0)\n",
    "                    current = content[1]\n",
    "                    YFull = pd.concat([YFull,current])\n",
    "                    page+=1\n",
    "                except ValueError:\n",
    "                    break\n",
    "        yfull = YFull[[\"SNP-ID\", \"Build38\", \"ANC\", \"DER\", \"Branch\"]]\n",
    "        yfull.columns = [[\"SNP-ID\", \"pos38\", \"ref\", \"var\", \"branch\"]]\n",
    "        print(\"yfull snps info updated!\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        isyb_snp = isyb[[\"Name\", \"allele_anc\", \"allele_der\", \"start\"]]\n",
    "        isyb_snp.columns = [\"snp\", \"ref\", \"var\", \"pos\"]\n",
    "        yfull_snp = yfull[[\"SNP-ID\", \"ANC\", \"DER\", \"Build38\"]]\n",
    "        yfull_snp.columns = [\"snp\", \"ref\", \"var\", \"pos\"]\n",
    "\n",
    "        isyb_intree = isyb_snp[isyb_snp[\"snp\"].isin(snps_unique)].drop_duplicates()\n",
    "        yfull_intree = yfull_snp[yfull_snp[\"snp\"].isin(snps_unique)].drop_duplicates()\n",
    "\n",
    "        isyb_yfull_intree = pd.concat([isyb_intree, yfull_intree]).drop_duplicates()\n",
    "        inspect_isyb_yfull_intree = isyb_yfull_intree.groupby(\"snp\").count().sort_values(by=\"ref\", ascending=False)\n",
    "        dup_list = inspect_isyb_yfull_intree[inspect_isyb_yfull_intree[\"ref\"]>=2].index\n",
    "        dups = isyb_yfull_intree[(isyb_yfull_intree[\"snp\"].isin(dup_list))].sort_values(by=\"snp\").reset_index(drop=True)\n",
    "        isyb_intree_fix = isyb_snp[(isyb_snp[\"snp\"].isin(snps_unique))&(isyb_snp[\"snp\"].isin(dup_list)==False)].drop_duplicates()\n",
    "        isyb_yfull_intree_fix = pd.concat([isyb_intree_fix, yfull_intree]).drop_duplicates()\n",
    "\n",
    "        missing = list(set(snps_unique).difference(set(isyb_yfull_intree_fix[\"snp\"].tolist())))\n",
    "\n",
    "        # update ybrowse search choice\n",
    "        dfs = {}\n",
    "        not_found = []\n",
    "        counter = 0\n",
    "        for snp in missing:\n",
    "            try: \n",
    "                htmlraw = requests.get(f\"https://ybrowse.org/gb2/gbrowse_details/chrY?ref=chrY;name={snp};class=Sequence;db_id=chrY%3Adatabase\")\n",
    "                df = pd.read_html(htmlraw.content, header=None)[0]\n",
    "                dfs[f\"{snp}\"]=df\n",
    "            except ValueError:\n",
    "                not_found.append(snp)\n",
    "\n",
    "        ybrowse_found = []\n",
    "        for key in dfs.keys():\n",
    "            df = dfs[key]\n",
    "            pos_yb = int(df.iloc[3][1].split(\":\")[1].split(\"..\")[0])\n",
    "            found_info = [df.iloc[0][1], df.iloc[5][1], df.iloc[6][1], pos_yb]\n",
    "            ybrowse_found.append(found_info)\n",
    "        ybrowse_found_df = pd.DataFrame(ybrowse_found)\n",
    "        ybrowse_found_df.columns = [\"snp\", \"ref\", \"var\", \"pos\"]\n",
    "\n",
    "        full_snps = pd.concat([isyb_yfull_intree_fix, ybrowse_found_df]).reset_index(drop=True)\n",
    "        \n",
    "        save = input(\"analysis database updated. overwrite the database for later use? [y/n]\")\n",
    "        if save==\"y\":\n",
    "            pickle.dump(tree, open(\"yfull_tree_info.sav\", 'wb'))\n",
    "            pickle.dump(full_snps, open(\"full_snps.sav\", 'wb'))\n",
    "            \n",
    "    # find variant matches\n",
    "    print(\"Analyzing...\")\n",
    "    matched = pd.DataFrame()\n",
    "    for i in range(len(var)):\n",
    "        varpos = var.iloc[i][\"GRCh38pos\"]\n",
    "        varvar = var.iloc[i][\"var\"]\n",
    "        varref = var.iloc[i][\"ref\"]\n",
    "        df = snps[(snps[\"pos\"]==varpos)&(snps[\"var\"].isin(varvar))&(snps[\"ref\"]==varref)]\n",
    "        matched = pd.concat([matched,df])\n",
    "    \n",
    "    print(f\"{len(matched)} variants matched\")\n",
    "    \n",
    "    # find yfull haplogroup match\n",
    "    matched = matched.reset_index(drop=True)\n",
    "    tree[\"match\"] = 0\n",
    "    tree.drop_duplicates(subset=[\"id\"], inplace=True)\n",
    "    \n",
    "    for i in matched.index:\n",
    "        matched_snp = matched.iloc[i][\"snp\"]\n",
    "        tree.loc[tree['snp_list'].apply(lambda x: matched_snp in x),\"match\"] += 1\n",
    "\n",
    "    # find scores\n",
    "    filtered_tree = tree[tree[\"match\"]>0]\n",
    "    filtered_tree = filtered_tree[[\"id\", \"match\", \"score\"]]\n",
    "\n",
    "    scores = filtered_tree.groupby(\"id\").sum()\n",
    "    scores[\"percent\"] = scores[\"match\"] * scores[\"score\"]\n",
    "    scores[\"log_score\"] = scores[\"percent\"] * np.log2(scores[\"match\"]+1)\n",
    "    scores[\"harmonic_score\"] = scores[\"percent\"] * 2/(scores[\"percent\"] + 1/scores[\"match\"])\n",
    "    \n",
    "    response = requests.get(\"https://raw.githubusercontent.com/YFullTeam/YTree/master/current_tree.json\")\n",
    "    json_data = json.loads(response.content)\n",
    "    def build_parent_map(node, parent_map, parent_id=None):\n",
    "        node_id = node.get('id')\n",
    "        parent_map[node_id] = parent_id\n",
    "        for child in node.get('children', []):\n",
    "            build_parent_map(child, parent_map, node_id)\n",
    "\n",
    "    def direct_path(node_id, parent_map, target_ids):\n",
    "        if node_id in parent_map and parent_map[node_id] in target_ids:\n",
    "            return [parent_map[node_id], node_id]\n",
    "        return []\n",
    "\n",
    "    def find_direct_paths(nodes_list, parent_map):\n",
    "        paths = []\n",
    "        for node_id in nodes_list:\n",
    "            if node_id in parent_map:  # Ensure the node is in the tree\n",
    "                path = direct_path(node_id, parent_map, nodes_list)\n",
    "                if path:  # Include only if a direct path is found\n",
    "                    paths.append(path)\n",
    "        return paths\n",
    "    # Main execution\n",
    "    if __name__ == \"__main__\":\n",
    "        parent_map = {}\n",
    "        build_parent_map(json_data, parent_map)  # Assuming json_data is your root node\n",
    "    \n",
    "    nodes_list = scores.index.to_list()  # Example list, replace with actual IDs\n",
    "    paths = find_direct_paths(nodes_list, parent_map)\n",
    "    \n",
    "    def merge_overlapping_paths(paths):\n",
    "        merged = True\n",
    "        while merged:\n",
    "            merged = False\n",
    "            for i in range(len(paths)):\n",
    "                for j in range(i + 1, len(paths)):\n",
    "                    # Check if the last node of path i is the first node of path j or vice versa\n",
    "                    if paths[i][-1] == paths[j][0]:\n",
    "                        paths[i].extend(paths[j][1:])\n",
    "                        paths.pop(j)\n",
    "                        merged = True\n",
    "                        break\n",
    "                    elif paths[i][0] == paths[j][-1]:\n",
    "                        paths[j].extend(paths[i][1:])\n",
    "                        paths.pop(i)\n",
    "                        merged = True\n",
    "                        break\n",
    "                if merged:\n",
    "                    break  # Restart scanning if any merge occurred\n",
    "        return paths\n",
    "    longest_paths = merge_overlapping_paths(paths)\n",
    "        \n",
    "    def visualize_paths_nx_vertical_with_longest_path_color(paths):\n",
    "        G = nx.DiGraph()\n",
    "        pos = {}  # Position mapping\n",
    "        x_offset = 0  # Horizontal offset for each path\n",
    "        longest_path = max(paths, key=len)  # Identify the longest path\n",
    "\n",
    "        # Setting up positions\n",
    "        for path in paths:\n",
    "            for i, node in enumerate(path):\n",
    "                pos[node] = (x_offset, -i)  # Assign position\n",
    "            x_offset += 1  # Increment x offset for each path\n",
    "\n",
    "        # Adding paths to the graph\n",
    "        for path in paths:\n",
    "            nx.add_path(G, path)\n",
    "\n",
    "        # Drawing\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        node_colors = [\"red\" if node in longest_path else \"skyblue\" for node in G.nodes()]\n",
    "        edge_colors = [\"red\" if (u, v) in zip(longest_path, longest_path[1:]) else \"black\" for u, v in G.edges()]\n",
    "\n",
    "        nx.draw(G, pos, with_labels=True, arrows=True, node_size=700, node_color=node_colors, \n",
    "                edge_color=edge_colors, font_size=10, font_weight=\"bold\", arrowstyle=\"->\", arrowsize=20)\n",
    "        plt.show()\n",
    "    \n",
    "    #show results\n",
    "    print(\"\\n\")\n",
    "    print(\"Bar graph below shows number of variants matched (>0) for each haplogroup:\")\n",
    "    plt.bar(height=scores[\"match\"].values, x=scores.index)\n",
    "    plt.title(\"Count matched variants\")\n",
    "    plt.xticks(rotation=90, fontsize=5.5)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Bar graph below shows percentage of variants matched (>0) for each haplogroup:\")\n",
    "    plt.bar(height=scores[\"percent\"].values, x=scores.index)\n",
    "    plt.title(\"Percentage(%) of variants matched\")\n",
    "    plt.xticks(rotation=90, fontsize=5.5)\n",
    "    plt.show()\n",
    "    print(\"percentage = matched_variants/variants_defining_haplogroup\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Bar graph below shows score1 (description below) of variants matched (>0) for each haplogroup:\")\n",
    "    plt.bar(height=scores[\"log_score\"].values, x=scores.index)\n",
    "    plt.title(\"Score1 (measure of fit to haplogroups)\")\n",
    "    plt.xticks(rotation=90, fontsize=5.5)\n",
    "    plt.show()\n",
    "    print(\"score1 = percentage * log(count + 1)\")\n",
    "    print(\"score1 aims to balance count and percentage of variants matched per haplogroup\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Bar graph below shows score2 (description below) of variants matched (>0) for each haplogroup:\")\n",
    "    plt.bar(height=scores[\"harmonic_score\"].values, x=scores.index)\n",
    "    plt.title(\"Score2 (measure of fit to haplogroups)\")\n",
    "    plt.xticks(rotation=90, fontsize=5.5)\n",
    "    plt.show()\n",
    "    print(\"score2 = percentage * (2 / (1/count + percentage))\")\n",
    "    print(\"score2 aims to balance count and percentage of variants matched per haplogroup\")\n",
    "    \n",
    "    print(\"Below arrow diagram shows the relatioship between matched haplogroups:\")\n",
    "    visualize_paths_nx_vertical_with_longest_path_color(longest_paths)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"According to the arrow diagrams, you most likely belong to the red path\")\n",
    "\n",
    "    longest = []\n",
    "    for path in longest_paths:\n",
    "        if len(path)>len(longest):\n",
    "            longest=path\n",
    "    haplo = longest[-1]\n",
    "    print(\"\\n\")\n",
    "    print(\"you most likely belog to same haplogroup with these samples\")\n",
    "    print(f\"https://www.yfull.com/tree/{haplo}\")\n",
    "    \n",
    "    return (matched, scores, filtered_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e10961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sample(sample, sample_name):\n",
    "    extracted = sample[[\"POS\", \"REF\", \"ALT\", sample_name]]\n",
    "    var_info = []\n",
    "    for i in extracted.index:\n",
    "        var_info.append(extracted.iloc[i][sample_name].split(\":\")[0])\n",
    "    extracted[\"var_info\"] = var_info\n",
    "    extracted_vars = extracted[extracted[\"var_info\"].isin([str(i) for i in range(1,10)])] \n",
    "    extracted_vars = extracted_vars.reset_index(drop=True)\n",
    "    alts = []\n",
    "    for i in extracted_vars.index:\n",
    "        alt = extracted_vars.iloc[i][\"ALT\"]\n",
    "        which = int(extracted_vars.iloc[i][\"var_info\"])-1\n",
    "        alt_ = alt.split(\",\")[which]\n",
    "        alts.append([alt_])\n",
    "    extracted_vars[\"var\"] = alts\n",
    "    output = extracted_vars[[\"POS\", \"REF\", \"var\"]]\n",
    "    pickle.dump(output, open(f\"{sample_name}.sav\", 'wb'))\n",
    "    \n",
    "    return output\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
