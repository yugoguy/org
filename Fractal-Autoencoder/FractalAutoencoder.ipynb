{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNeOqrkNgpxcvi5UywVerXf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":22,"metadata":{"id":"hRZoQO2d1XJ5","executionInfo":{"status":"ok","timestamp":1757852080810,"user_tz":-540,"elapsed":9,"user":{"displayName":"Yugo Nagatake","userId":"15348269876584780739"}}},"outputs":[],"source":["\"\"\"\n","Fractal Network Configuration\n","\"\"\"\n","\n","from dataclasses import dataclass\n","from typing import List, Optional\n","\n","\n","@dataclass\n","class FractalConfig:\n","    \"\"\"Fractal Networkのハイパーパラメータ設定\"\"\"\n","\n","    # 定数パラメータ\n","    max_depth: int = 3\n","    input_dim: int = 128\n","    output_dim: int = 10\n","    num_iterations: int = 10  # PrototypeMatchingの反復回数\n","    gate_momentum: float = 0.5  # ゲート更新の運動量係数\n","    use_ffn: bool = True  # BranchAttentionでFFN使用\n","    attention_heads: int = 8  # Decoder側のMulti-head attention数\n","\n","    # depthごとのパラメータ（リスト形式）\n","    head_dims: List[int] = None  # 各depthの分岐数 H\n","    proto_dims: List[int] = None  # 各depthのプロトタイプ次元 D\n","    proto_nums: List[int] = None  # 各depthのプロトタイプ数 T\n","\n","    def __post_init__(self):\n","        \"\"\"デフォルト値の設定と検証\"\"\"\n","        # デフォルト値設定\n","        if self.head_dims is None:\n","            self.head_dims = [8] * self.max_depth\n","        if self.proto_dims is None:\n","            self.proto_dims = [64] * self.max_depth\n","        if self.proto_nums is None:\n","            self.proto_nums = [16] * self.max_depth\n","\n","        # 長さの検証\n","        assert len(self.head_dims) == self.max_depth, \\\n","            f\"head_dims length {len(self.head_dims)} != max_depth {self.max_depth}\"\n","        assert len(self.proto_dims) == self.max_depth, \\\n","            f\"proto_dims length {len(self.proto_dims)} != max_depth {self.max_depth}\"\n","        assert len(self.proto_nums) == self.max_depth, \\\n","            f\"proto_nums length {len(self.proto_nums)} != max_depth {self.max_depth}\"\n","\n","    def get_ffn_hidden_dim(self, depth: int) -> int:\n","        \"\"\"FFNの中間次元を取得（入力次元の2倍）\"\"\"\n","        return self.proto_dims[depth] * 2\n","\n","    def get_input_dim_for_depth(self, depth: int) -> int:\n","        \"\"\"各depthの入力次元を取得\"\"\"\n","        if depth == 0:\n","            return self.input_dim\n","        else:\n","            return self.proto_dims[depth - 1]\n","\n","\n","# デフォルト設定の例\n","def get_default_config():\n","    \"\"\"デフォルト設定を返す\"\"\"\n","    return FractalConfig(\n","        max_depth=3,\n","        input_dim=128,\n","        output_dim=10,\n","        num_iterations=10,\n","        gate_momentum=0.5,\n","        use_ffn=True,\n","        attention_heads=8,\n","        head_dims=[8, 16, 32],\n","        proto_dims=[64, 128, 256],\n","        proto_nums=[16, 32, 64]\n","    )\n","\n","\n","# # 使用例\n","# if __name__ == \"__main__\":\n","#     config = get_default_config()\n","#     print(f\"Config created with max_depth={config.max_depth}\")\n","#     print(f\"FFN hidden dim at depth 0: {config.get_ffn_hidden_dim(0)}\")\n","#     print(f\"Input dim for depth 1: {config.get_input_dim_for_depth(1)}\")"]},{"cell_type":"code","source":["\"\"\"\n","Fractal Encoder Module\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from typing import Dict, Tuple, Optional\n","from dataclasses import dataclass\n","\n","\n","@dataclass\n","class NodeID:\n","    \"\"\"ノードを識別するためのデータクラス\"\"\"\n","    depth: int\n","    parent_id: Optional[Tuple] = None\n","    node_id: int = 0\n","\n","    def to_key(self):\n","        \"\"\"辞書のキーとして使用\"\"\"\n","        return (self.depth, self.parent_id, self.node_id)\n","\n","\n","class PrototypeMatching(nn.Module):\n","    \"\"\"プロトタイプベースのマッチングメカニズム（PaQ/PaK）\"\"\"\n","\n","    def __init__(self, input_dim: int, proto_dim: int, proto_num: int, head_dim: int):\n","        \"\"\"\n","        Args:\n","            input_dim: 入力次元\n","            proto_dim: プロトタイプ次元 D\n","            proto_num: プロトタイプ数 T\n","            head_dim: ヘッド数（分岐数）H\n","        \"\"\"\n","        super().__init__()\n","        self.input_dim = input_dim\n","        self.proto_dim = proto_dim\n","        self.proto_num = proto_num\n","        self.head_dim = head_dim\n","\n","        # 重み行列（PaQ用）\n","        self.Wxq = nn.Parameter(torch.randn(head_dim, input_dim, proto_dim))\n","        self.Wxk = nn.Parameter(torch.randn(head_dim, input_dim, proto_dim))\n","        self.Wxv = nn.Parameter(torch.randn(head_dim, input_dim, proto_dim))\n","        self.Wpq = nn.Parameter(torch.randn(head_dim, proto_num, proto_dim, proto_dim))\n","        self.Wpk = nn.Parameter(torch.randn(head_dim, proto_num, proto_dim, proto_dim))\n","\n","        # プロトタイプベクトル（学習可能）\n","        self.prototypes = nn.Parameter(torch.randn(head_dim, proto_num, proto_dim))\n","\n","        self._init_weights()\n","\n","    def _init_weights(self):\n","        \"\"\"重みの初期化\"\"\"\n","        for param in [self.Wxq, self.Wxk, self.Wxv]:\n","            nn.init.xavier_uniform_(param)\n","        for param in [self.Wpq, self.Wpk]:\n","            nn.init.xavier_uniform_(param.view(self.head_dim * self.proto_num, -1))\n","        nn.init.xavier_uniform_(self.prototypes.view(self.head_dim * self.proto_num, -1))\n","\n","    def forward(self, x: torch.Tensor, gate: torch.Tensor,\n","                num_iterations: int = 10, momentum: float = 0.5) -> torch.Tensor:\n","        \"\"\"\n","        Args:\n","            x: 入力 [batch, input_dim]\n","            gate: ゲート初期値 [batch, head_dim, proto_num]\n","            num_iterations: 反復回数\n","            momentum: ゲート更新の運動量\n","\n","        Returns:\n","            出力 [batch, head_dim, proto_dim]\n","        \"\"\"\n","        batch_size = x.shape[0]\n","\n","        # 入力を拡張 [batch, head_dim, input_dim]\n","        x_expanded = x.unsqueeze(1).expand(-1, self.head_dim, -1)\n","\n","        # プロトタイプとゲートを初期化\n","        P = self.prototypes.unsqueeze(0).expand(batch_size, -1, -1, -1)  # [batch, H, T, D]\n","        G = gate  # [batch, H, T]\n","\n","        # 反復的な更新\n","        for _ in range(num_iterations):\n","            # PaK: ゲート更新\n","            Qx = torch.einsum('hid,bhi->bhd', self.Wxq, x_expanded)  # [batch, H, D]\n","            Kp = torch.einsum('htdd,bhtd->bhtd', self.Wpk, P)  # [batch, H, T, D]\n","            Gx = F.softmax(torch.einsum('bhd,bhtd->bht', Qx, Kp), dim=-1)  # [batch, H, T]\n","\n","            # PaQ: プロトタイプ更新\n","            Vx = torch.einsum('hid,bhi->bhd', self.Wxv, x_expanded)  # [batch, H, D]\n","            Kx = torch.einsum('hid,bhi->bhd', self.Wxk, x_expanded)  # [batch, H, D]\n","            Qp = torch.einsum('htdd,bhtd->bhtd', self.Wpq, P)  # [batch, H, T, D]\n","\n","            # Attention scores and values\n","            scores = torch.einsum('bhtd,bhd->bht', Qp, Kx)  # [batch, H, T]\n","            attn = F.softmax(scores, dim=-1)  # [batch, H, T]\n","            Ap = torch.einsum('bht,bhd->bhtd', attn, Vx)  # [batch, H, T, D]\n","\n","            # ゲートとプロトタイプの更新\n","            G = (1 - momentum) * G + momentum * Gx\n","            P = torch.einsum('bht,bhtd->bhtd', G, P + Ap)\n","\n","        # プロトタイプ数次元でsum\n","        output = P.sum(dim=2)  # [batch, H, D]\n","        return F.relu(output)\n","\n","\n","class FractalEncoder(nn.Module):\n","    \"\"\"フラクタルエンコーダ\"\"\"\n","\n","    def __init__(self, config):\n","        \"\"\"\n","        Args:\n","            config: FractalConfig\n","        \"\"\"\n","        super().__init__()\n","        self.config = config\n","\n","        # 各depthのPrototypeMatchingモジュール\n","        self.prototype_matching = nn.ModuleList()\n","        for depth in range(config.max_depth):\n","            input_dim = config.get_input_dim_for_depth(depth)\n","            self.prototype_matching.append(\n","                PrototypeMatching(\n","                    input_dim=input_dim,\n","                    proto_dim=config.proto_dims[depth],\n","                    proto_num=config.proto_nums[depth],\n","                    head_dim=config.head_dims[depth]\n","                )\n","            )\n","\n","        # ゲート初期値（各ノードごと）\n","        self.gate_init = nn.ParameterDict()\n","        self._init_gates()\n","\n","    def _init_gates(self):\n","        \"\"\"ゲート初期値を再帰的に初期化\"\"\"\n","        def register_gate(depth, parent_id, node_id):\n","            if depth >= self.config.max_depth:\n","                return\n","\n","            key = f\"d{depth}_p{parent_id}_n{node_id}\"\n","            self.gate_init[key] = nn.Parameter(\n","                torch.randn(self.config.head_dims[depth], self.config.proto_nums[depth])\n","            )\n","\n","            # 子ノードのゲートも登録\n","            for h in range(self.config.head_dims[depth]):\n","                child_parent_id = f\"{parent_id}_{node_id}\" if parent_id else str(node_id)\n","                register_gate(depth + 1, child_parent_id, h)\n","\n","        register_gate(0, None, 0)\n","\n","    def _get_gate_key(self, depth: int, parent_id: Optional[str], node_id: int) -> str:\n","        \"\"\"ゲートのキーを生成\"\"\"\n","        return f\"d{depth}_p{parent_id}_n{node_id}\"\n","\n","    def forward(self, x: torch.Tensor) -> Dict[Tuple, torch.Tensor]:\n","        \"\"\"\n","        Args:\n","            x: 入力 [batch, input_dim]\n","\n","        Returns:\n","            features_dict: 各ノードの特徴量を格納した辞書\n","                key: (depth, parent_id, node_id)\n","                value: [batch, proto_dim]\n","        \"\"\"\n","        batch_size = x.shape[0]\n","        features_dict = {}\n","\n","        def fractalize(x_node, depth, parent_id, node_id):\n","            \"\"\"再帰的なfractalize処理\"\"\"\n","            # 現在のノードの特徴を保存\n","            node_key = (depth, parent_id, node_id)\n","            features_dict[node_key] = x_node\n","\n","            if depth >= self.config.max_depth:\n","                return\n","\n","            # ゲート初期値を取得\n","            gate_key = self._get_gate_key(depth, parent_id, node_id)\n","            gate = self.gate_init[gate_key].unsqueeze(0).expand(batch_size, -1, -1)\n","\n","            # PrototypeMatchingで分岐\n","            children = self.prototype_matching[depth](\n","                x_node, gate,\n","                num_iterations=self.config.num_iterations,\n","                momentum=self.config.gate_momentum\n","            )  # [batch, H, D]\n","\n","            # 各子ノードを再帰的に処理\n","            for h in range(self.config.head_dims[depth]):\n","                child_parent_id = f\"{parent_id}_{node_id}\" if parent_id else str(node_id)\n","                fractalize(\n","                    children[:, h, :],  # [batch, D]\n","                    depth + 1,\n","                    child_parent_id,\n","                    h\n","                )\n","\n","        # ルートから開始\n","        fractalize(x, 0, None, 0)\n","\n","        return features_dict"],"metadata":{"id":"IjrjDqVTD6Nn","executionInfo":{"status":"ok","timestamp":1757852081162,"user_tz":-540,"elapsed":36,"user":{"displayName":"Yugo Nagatake","userId":"15348269876584780739"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Fractal Decoder Module\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from typing import Dict, Tuple, List, Optional\n","\n","\n","class BranchAttention(nn.Module):\n","    \"\"\"ブランチごとのAttention処理（MHA + FFN）\"\"\"\n","\n","    def __init__(self, dim: int, num_heads: int, use_ffn: bool = True):\n","        \"\"\"\n","        Args:\n","            dim: 入力次元\n","            num_heads: アテンションヘッド数\n","            use_ffn: FFN使用フラグ\n","        \"\"\"\n","        super().__init__()\n","        self.dim = dim\n","        self.num_heads = num_heads\n","        self.use_ffn = use_ffn\n","\n","        assert dim % num_heads == 0, f\"dim {dim} must be divisible by num_heads {num_heads}\"\n","        self.head_dim = dim // num_heads\n","\n","        # Multi-head Attention\n","        self.q_proj = nn.Linear(dim, dim)\n","        self.k_proj = nn.Linear(dim, dim)\n","        self.v_proj = nn.Linear(dim, dim)\n","        self.out_proj = nn.Linear(dim, dim)\n","\n","        # FFN (2層、中間次元は入力の2倍)\n","        if use_ffn:\n","            hidden_dim = dim * 2\n","            self.ffn = nn.Sequential(\n","                nn.Linear(dim, hidden_dim),\n","                nn.ReLU(),\n","                nn.Linear(hidden_dim, dim)\n","            )\n","\n","        self.layer_norm1 = nn.LayerNorm(dim)\n","        self.layer_norm2 = nn.LayerNorm(dim) if use_ffn else None\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Args:\n","            x: 入力 [batch, num_tokens, dim]\n","\n","        Returns:\n","            出力 [batch, num_tokens, dim]\n","        \"\"\"\n","        batch_size, num_tokens, _ = x.shape\n","\n","        # Multi-head Attention\n","        residual = x\n","        x = self.layer_norm1(x)\n","\n","        # Q, K, V projection and reshape for multi-head\n","        q = self.q_proj(x).view(batch_size, num_tokens, self.num_heads, self.head_dim)\n","        k = self.k_proj(x).view(batch_size, num_tokens, self.num_heads, self.head_dim)\n","        v = self.v_proj(x).view(batch_size, num_tokens, self.num_heads, self.head_dim)\n","\n","        # Transpose for attention: [batch, heads, tokens, head_dim]\n","        q = q.transpose(1, 2)\n","        k = k.transpose(1, 2)\n","        v = v.transpose(1, 2)\n","\n","        # Attention scores\n","        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n","        attn = F.softmax(scores, dim=-1)\n","\n","        # Apply attention to values\n","        out = torch.matmul(attn, v)  # [batch, heads, tokens, head_dim]\n","\n","        # Concatenate heads\n","        out = out.transpose(1, 2).contiguous().view(batch_size, num_tokens, self.dim)\n","        out = self.out_proj(out)\n","\n","        # Residual connection\n","        x = residual + out\n","\n","        # FFN\n","        if self.use_ffn:\n","            residual = x\n","            x = self.layer_norm2(x)\n","            x = residual + self.ffn(x)\n","\n","        return x\n","\n","\n","class Defractalize(nn.Module):\n","    \"\"\"トークンを統合して次元削減（token conv）\"\"\"\n","\n","    def __init__(self, token_dim: int, output_dim: int):\n","        \"\"\"\n","        Args:\n","            token_dim: トークンの次元 d_n\n","            output_dim: 出力次元 d_{n-1}\n","        \"\"\"\n","        super().__init__()\n","        self.token_dim = token_dim\n","        self.output_dim = output_dim\n","\n","        # Conv: tokens -> single vector\n","        self.token_conv = nn.Conv1d(token_dim, output_dim, kernel_size=1)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Args:\n","            x: 入力 [batch, num_tokens, token_dim]\n","\n","        Returns:\n","            出力 [batch, output_dim]\n","        \"\"\"\n","        # [batch, num_tokens, token_dim] -> [batch, token_dim, num_tokens]\n","        x = x.transpose(1, 2)\n","\n","        # Conv1d: [batch, token_dim, num_tokens] -> [batch, output_dim, num_tokens]\n","        x = self.token_conv(x)\n","\n","        # Mean pooling over tokens\n","        x = x.mean(dim=-1)  # [batch, output_dim]\n","\n","        return x\n","\n","\n","class ResidualConnection(nn.Module):\n","    \"\"\"エンコーダ特徴量との残差接続\"\"\"\n","\n","    def __init__(self, decoder_dim: int, encoder_dim: int, output_dim: int):\n","        \"\"\"\n","        Args:\n","            decoder_dim: デコーダ特徴量次元\n","            encoder_dim: エンコーダ特徴量次元\n","            output_dim: 出力次元\n","        \"\"\"\n","        super().__init__()\n","        self.decoder_dim = decoder_dim\n","        self.encoder_dim = encoder_dim\n","        self.output_dim = output_dim\n","\n","        # Linear transformation for residual\n","        self.residual_conv = nn.Linear(decoder_dim + encoder_dim, output_dim)\n","\n","    def forward(self, decoder_features: torch.Tensor,\n","                encoder_features: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Args:\n","            decoder_features: デコーダ特徴量 [batch, decoder_dim]\n","            encoder_features: エンコーダ特徴量 [batch, encoder_dim]\n","\n","        Returns:\n","            統合された特徴量 [batch, output_dim]\n","        \"\"\"\n","        # Concatenate and transform\n","        combined = torch.cat([decoder_features, encoder_features], dim=-1)\n","        output = self.residual_conv(combined)\n","        return F.relu(output)\n","\n","\n","class FractalDecoder(nn.Module):\n","    \"\"\"フラクタルデコーダ\"\"\"\n","\n","    def __init__(self, config):\n","        \"\"\"\n","        Args:\n","            config: FractalConfig\n","        \"\"\"\n","        super().__init__()\n","        self.config = config\n","\n","        # 各depthのモジュール\n","        self.branch_attention = nn.ModuleList()\n","        self.defractalize = nn.ModuleList()\n","        self.residual_connection = nn.ModuleList()\n","\n","        # 各depth（0からmax_depth-1）で子ノードを統合するモジュールを作成\n","        for depth in range(config.max_depth):\n","            # 子ノードから来る特徴量の次元\n","            # depth=0の子（depth=1）はproto_dims[0]を返す\n","            # depth=1の子（depth=2）はproto_dims[1]を返す\n","            # depth=max_depth-1の子（最深部）はproto_dims[max_depth-1]を返す\n","            if depth < config.max_depth - 1:\n","                child_dim = config.proto_dims[depth]\n","            else:\n","                # 最深部の親：子は最深部なのでproto_dims[max_depth-1]\n","                child_dim = config.proto_dims[config.max_depth - 1]\n","\n","            # BranchAttention：子ノードの次元に対して\n","            self.branch_attention.append(\n","                BranchAttention(\n","                    dim=child_dim,\n","                    num_heads=config.attention_heads,\n","                    use_ffn=config.use_ffn\n","                )\n","            )\n","\n","            # Defractalize：子ノード次元から現在のノードが必要とする次元へ\n","            if depth > 0:\n","                output_dim = config.proto_dims[depth - 1]\n","            else:\n","                # ルート：子ノードproto_dims[0]からinput_dimへ\n","                output_dim = config.input_dim\n","\n","            self.defractalize.append(\n","                Defractalize(\n","                    token_dim=child_dim,\n","                    output_dim=output_dim\n","                )\n","            )\n","\n","            # ResidualConnection：デコーダとエンコーダの特徴量を結合\n","            if depth == 0:\n","                # ルート：\n","                # decoder: input_dim, encoder: input_dim -> output: input_dim\n","                self.residual_connection.append(\n","                    ResidualConnection(\n","                        decoder_dim=config.input_dim,\n","                        encoder_dim=config.input_dim,\n","                        output_dim=config.input_dim\n","                    )\n","                )\n","            else:\n","                # その他：同じ次元\n","                dim = config.proto_dims[depth - 1]\n","                self.residual_connection.append(\n","                    ResidualConnection(\n","                        decoder_dim=dim,\n","                        encoder_dim=dim,\n","                        output_dim=dim\n","                    )\n","                )\n","\n","    def forward(self, encoder_features: Dict[Tuple, torch.Tensor]) -> torch.Tensor:\n","        \"\"\"\n","        Args:\n","            encoder_features: エンコーダの特徴量辞書\n","                key: (depth, parent_id, node_id)\n","                value: [batch, proto_dim]\n","\n","        Returns:\n","            出力特徴量 [batch, input_dim]\n","        \"\"\"\n","        # Bottom-up処理用の辞書\n","        decoder_features = {}\n","\n","        def process_node(depth, parent_id, node_id):\n","            \"\"\"ノードをbottom-upで処理\"\"\"\n","            node_key = (depth, parent_id, node_id)\n","\n","            if depth == self.config.max_depth:\n","                # 最深部：エンコーダ特徴量をそのまま返す（Attention不要）\n","                features = encoder_features[node_key]\n","                decoder_features[node_key] = features\n","                return features\n","\n","            # 子ノードの特徴を収集\n","            children_features = []\n","            for h in range(self.config.head_dims[depth]):\n","                child_parent_id = f\"{parent_id}_{node_id}\" if parent_id else str(node_id)\n","                child_key = (depth + 1, child_parent_id, h)\n","\n","                if child_key in encoder_features:\n","                    child_feat = process_node(depth + 1, child_parent_id, h)\n","                    children_features.append(child_feat)\n","\n","            if not children_features:\n","                # 子ノードがない場合（通常あり得ない）\n","                return encoder_features[node_key]\n","\n","            # 子ノードの特徴をスタック\n","            children_tensor = torch.stack(children_features, dim=1)  # [batch, H, D_child]\n","\n","            # BranchAttention（depthのモジュールを使用）\n","            children_tensor = self.branch_attention[depth](children_tensor)\n","\n","            # Defractalize: [batch, H, D_child] -> [batch, D_current]\n","            integrated = self.defractalize[depth](children_tensor)\n","\n","            # Residual connection with encoder features\n","            encoder_feat = encoder_features[node_key]\n","            output = self.residual_connection[depth](integrated, encoder_feat)\n","\n","            decoder_features[node_key] = output\n","            return output\n","\n","        # ルートノードから処理開始（再帰的にbottom-up）\n","        root_features = process_node(0, None, 0)\n","\n","        return root_features"],"metadata":{"id":"9ZfrtSMtD64U","executionInfo":{"status":"ok","timestamp":1757852081519,"user_tz":-540,"elapsed":42,"user":{"displayName":"Yugo Nagatake","userId":"15348269876584780739"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Fractal Autoencoder - 全体統合モジュール\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","from typing import Dict, Tuple, Optional\n","\n","\n","class OutputHead(nn.Module):\n","    \"\"\"最終出力層\"\"\"\n","\n","    def __init__(self, input_dim: int, output_dim: int):\n","        \"\"\"\n","        Args:\n","            input_dim: 入力次元（Decoderからのinput_dim）\n","            output_dim: 最終出力次元\n","        \"\"\"\n","        super().__init__()\n","        self.fc = nn.Linear(input_dim, output_dim)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Args:\n","            x: 入力 [batch, input_dim]\n","\n","        Returns:\n","            出力 [batch, output_dim]\n","        \"\"\"\n","        return self.fc(x)\n","\n","\n","class FractalAutoencoder(nn.Module):\n","    \"\"\"フラクタルオートエンコーダ全体\"\"\"\n","\n","    def __init__(self, config):\n","        \"\"\"\n","        Args:\n","            config: FractalConfig\n","        \"\"\"\n","        super().__init__()\n","        self.config = config\n","\n","        # モジュール\n","        self.encoder = FractalEncoder(config)\n","        self.decoder = FractalDecoder(config)\n","        self.output_head = OutputHead(\n","            input_dim=config.input_dim,  # Decoderはinput_dimまで戻す\n","            output_dim=config.output_dim\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Args:\n","            x: 入力 [batch, input_dim]\n","\n","        Returns:\n","            出力 [batch, output_dim]\n","        \"\"\"\n","        # Encode\n","        encoder_features = self.encoder(x)\n","\n","        # Decode\n","        decoded_features = self.decoder(encoder_features)\n","\n","        # Output projection\n","        output = self.output_head(decoded_features)\n","\n","        return output\n","\n","    def get_encoder_features(self, x: torch.Tensor) -> Dict[Tuple, torch.Tensor]:\n","        \"\"\"エンコーダの中間特徴量を取得（デバッグ用）\"\"\"\n","        return self.encoder(x)\n","\n","    def decode_from_features(self, encoder_features: Dict[Tuple, torch.Tensor]) -> torch.Tensor:\n","        \"\"\"特徴量から直接デコード（デバッグ用）\"\"\"\n","        decoded = self.decoder(encoder_features)\n","        return self.output_head(decoded)\n","\n","\n","# 使用例とテストコード\n","def test_fractal_autoencoder():\n","    \"\"\"動作確認用のテスト関数\"\"\"\n","    # 設定\n","    config = get_default_config()\n","    config.max_depth = 2  # テスト用に浅くする\n","    config.input_dim = 64\n","    config.output_dim = 10\n","    config.head_dims = [4, 8]\n","    config.proto_dims = [32, 64]\n","    config.proto_nums = [8, 16]\n","\n","    # モデル作成\n","    model = FractalAutoencoder(config)\n","\n","    # ダミー入力\n","    batch_size = 2\n","    x = torch.randn(batch_size, config.input_dim)\n","\n","    # Forward pass\n","    output = model(x)\n","    print(f\"Input shape: {x.shape}\")\n","    print(f\"Output shape: {output.shape}\")\n","\n","    # エンコーダ特徴量の確認\n","    encoder_features = model.get_encoder_features(x)\n","    print(f\"\\nEncoder features:\")\n","    for key, value in encoder_features.items():\n","        print(f\"  Node {key}: shape {value.shape}\")\n","\n","    # パラメータ数\n","    total_params = sum(p.numel() for p in model.parameters())\n","    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(f\"\\nTotal parameters: {total_params:,}\")\n","    print(f\"Trainable parameters: {trainable_params:,}\")\n","\n","    return model\n","\n","\n","# デバッグ用：勾配チェック\n","def check_gradients(model, x, target):\n","    \"\"\"勾配が正しく流れるかチェック\"\"\"\n","    criterion = nn.MSELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","\n","    # Forward\n","    output = model(x)\n","    loss = criterion(output, target)\n","\n","    # Backward\n","    optimizer.zero_grad()\n","    loss.backward()\n","\n","    # 勾配チェック\n","    print(\"\\nGradient check:\")\n","    for name, param in model.named_parameters():\n","        if param.grad is not None:\n","            grad_norm = param.grad.norm().item()\n","            print(f\"  {name}: grad_norm = {grad_norm:.6f}\")\n","            if grad_norm == 0:\n","                print(f\"    WARNING: Zero gradient!\")\n","\n","    return loss.item()\n","\n"],"metadata":{"id":"KFov1FNjD9ck","executionInfo":{"status":"ok","timestamp":1757852496176,"user_tz":-540,"elapsed":14,"user":{"displayName":"Yugo Nagatake","userId":"15348269876584780739"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# if __name__ == \"__main__\":\n","#     # テスト実行\n","#     model = test_fractal_autoencoder()\n","\n","#     # 勾配チェック\n","#     batch_size = 2\n","#     x = torch.randn(batch_size, model.config.input_dim)\n","#     target = torch.randn(batch_size, model.config.output_dim)\n","#     loss = check_gradients(model, x, target)\n","#     print(f\"\\nLoss: {loss:.6f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T7OCP81yEIWS","executionInfo":{"status":"ok","timestamp":1757852505687,"user_tz":-540,"elapsed":7150,"user":{"displayName":"Yugo Nagatake","userId":"15348269876584780739"}},"outputId":"8550bde0-5166-4dcc-d58a-f9653cbefcc1"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Input shape: torch.Size([2, 64])\n","Output shape: torch.Size([2, 10])\n","\n","Encoder features:\n","  Node (0, None, 0): shape torch.Size([2, 64])\n","  Node (1, '0', 0): shape torch.Size([2, 32])\n","  Node (2, '0_0', 0): shape torch.Size([2, 64])\n","  Node (2, '0_0', 1): shape torch.Size([2, 64])\n","  Node (2, '0_0', 2): shape torch.Size([2, 64])\n","  Node (2, '0_0', 3): shape torch.Size([2, 64])\n","  Node (2, '0_0', 4): shape torch.Size([2, 64])\n","  Node (2, '0_0', 5): shape torch.Size([2, 64])\n","  Node (2, '0_0', 6): shape torch.Size([2, 64])\n","  Node (2, '0_0', 7): shape torch.Size([2, 64])\n","  Node (1, '0', 1): shape torch.Size([2, 32])\n","  Node (2, '0_1', 0): shape torch.Size([2, 64])\n","  Node (2, '0_1', 1): shape torch.Size([2, 64])\n","  Node (2, '0_1', 2): shape torch.Size([2, 64])\n","  Node (2, '0_1', 3): shape torch.Size([2, 64])\n","  Node (2, '0_1', 4): shape torch.Size([2, 64])\n","  Node (2, '0_1', 5): shape torch.Size([2, 64])\n","  Node (2, '0_1', 6): shape torch.Size([2, 64])\n","  Node (2, '0_1', 7): shape torch.Size([2, 64])\n","  Node (1, '0', 2): shape torch.Size([2, 32])\n","  Node (2, '0_2', 0): shape torch.Size([2, 64])\n","  Node (2, '0_2', 1): shape torch.Size([2, 64])\n","  Node (2, '0_2', 2): shape torch.Size([2, 64])\n","  Node (2, '0_2', 3): shape torch.Size([2, 64])\n","  Node (2, '0_2', 4): shape torch.Size([2, 64])\n","  Node (2, '0_2', 5): shape torch.Size([2, 64])\n","  Node (2, '0_2', 6): shape torch.Size([2, 64])\n","  Node (2, '0_2', 7): shape torch.Size([2, 64])\n","  Node (1, '0', 3): shape torch.Size([2, 32])\n","  Node (2, '0_3', 0): shape torch.Size([2, 64])\n","  Node (2, '0_3', 1): shape torch.Size([2, 64])\n","  Node (2, '0_3', 2): shape torch.Size([2, 64])\n","  Node (2, '0_3', 3): shape torch.Size([2, 64])\n","  Node (2, '0_3', 4): shape torch.Size([2, 64])\n","  Node (2, '0_3', 5): shape torch.Size([2, 64])\n","  Node (2, '0_3', 6): shape torch.Size([2, 64])\n","  Node (2, '0_3', 7): shape torch.Size([2, 64])\n","\n","Total parameters: 1,254,794\n","Trainable parameters: 1,254,794\n","\n","Gradient check:\n","  encoder.prototype_matching.0.Wxq: grad_norm = 0.000000\n","  encoder.prototype_matching.0.Wxk: grad_norm = 0.000000\n","  encoder.prototype_matching.0.Wxv: grad_norm = 0.029622\n","  encoder.prototype_matching.0.Wpq: grad_norm = 0.000000\n","  encoder.prototype_matching.0.Wpk: grad_norm = 0.000000\n","  encoder.prototype_matching.0.prototypes: grad_norm = 0.000000\n","  encoder.prototype_matching.1.Wxq: grad_norm = 0.000000\n","  encoder.prototype_matching.1.Wxk: grad_norm = 0.000000\n","  encoder.prototype_matching.1.Wxv: grad_norm = 0.012828\n","  encoder.prototype_matching.1.Wpq: grad_norm = 0.000000\n","  encoder.prototype_matching.1.Wpk: grad_norm = 0.000000\n","  encoder.prototype_matching.1.prototypes: grad_norm = 0.000000\n","  encoder.gate_init.d0_pNone_n0: grad_norm = 0.000002\n","  encoder.gate_init.d1_p0_n0: grad_norm = 0.000001\n","  encoder.gate_init.d1_p0_n1: grad_norm = 0.000001\n","  encoder.gate_init.d1_p0_n2: grad_norm = 0.000000\n","  encoder.gate_init.d1_p0_n3: grad_norm = 0.000000\n","  decoder.branch_attention.0.q_proj.weight: grad_norm = 0.000322\n","  decoder.branch_attention.0.q_proj.bias: grad_norm = 0.000057\n","  decoder.branch_attention.0.k_proj.weight: grad_norm = 0.000398\n","  decoder.branch_attention.0.k_proj.bias: grad_norm = 0.000000\n","  decoder.branch_attention.0.v_proj.weight: grad_norm = 0.190513\n","  decoder.branch_attention.0.v_proj.bias: grad_norm = 0.033836\n","  decoder.branch_attention.0.out_proj.weight: grad_norm = 0.219852\n","  decoder.branch_attention.0.out_proj.bias: grad_norm = 0.063645\n","  decoder.branch_attention.0.ffn.0.weight: grad_norm = 0.133031\n","  decoder.branch_attention.0.ffn.0.bias: grad_norm = 0.023525\n","  decoder.branch_attention.0.ffn.2.weight: grad_norm = 0.175117\n","  decoder.branch_attention.0.ffn.2.bias: grad_norm = 0.050802\n","  decoder.branch_attention.0.layer_norm1.weight: grad_norm = 0.018135\n","  decoder.branch_attention.0.layer_norm1.bias: grad_norm = 0.017739\n","  decoder.branch_attention.0.layer_norm2.weight: grad_norm = 0.009428\n","  decoder.branch_attention.0.layer_norm2.bias: grad_norm = 0.010466\n","  decoder.branch_attention.1.q_proj.weight: grad_norm = 0.000001\n","  decoder.branch_attention.1.q_proj.bias: grad_norm = 0.000008\n","  decoder.branch_attention.1.k_proj.weight: grad_norm = 0.000008\n","  decoder.branch_attention.1.k_proj.bias: grad_norm = 0.000000\n","  decoder.branch_attention.1.v_proj.weight: grad_norm = 0.004253\n","  decoder.branch_attention.1.v_proj.bias: grad_norm = 0.058088\n","  decoder.branch_attention.1.out_proj.weight: grad_norm = 0.067033\n","  decoder.branch_attention.1.out_proj.bias: grad_norm = 0.109748\n","  decoder.branch_attention.1.ffn.0.weight: grad_norm = 0.133059\n","  decoder.branch_attention.1.ffn.0.bias: grad_norm = 0.016661\n","  decoder.branch_attention.1.ffn.2.weight: grad_norm = 0.166537\n","  decoder.branch_attention.1.ffn.2.bias: grad_norm = 0.038432\n","  decoder.branch_attention.1.layer_norm1.weight: grad_norm = 0.000325\n","  decoder.branch_attention.1.layer_norm1.bias: grad_norm = 0.031736\n","  decoder.branch_attention.1.layer_norm2.weight: grad_norm = 0.009919\n","  decoder.branch_attention.1.layer_norm2.bias: grad_norm = 0.008952\n","  decoder.defractalize.0.token_conv.weight: grad_norm = 0.185045\n","  decoder.defractalize.0.token_conv.bias: grad_norm = 0.083078\n","  decoder.defractalize.1.token_conv.weight: grad_norm = 0.158699\n","  decoder.defractalize.1.token_conv.bias: grad_norm = 0.071612\n","  decoder.residual_connection.0.residual_conv.weight: grad_norm = 1.510780\n","  decoder.residual_connection.0.residual_conv.bias: grad_norm = 0.212191\n","  decoder.residual_connection.1.residual_conv.weight: grad_norm = 0.196904\n","  decoder.residual_connection.1.residual_conv.bias: grad_norm = 0.179095\n","  output_head.fc.weight: grad_norm = 1.124245\n","  output_head.fc.bias: grad_norm = 0.405762\n","\n","Loss: 0.792655\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"wFokrx43JiB5"},"execution_count":null,"outputs":[]}]}