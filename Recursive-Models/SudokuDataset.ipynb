{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNVpty7dJIz9guUMT4n37kE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","from datasets import load_dataset\n","import numpy as np\n","from tqdm import tqdm\n","import pickle\n","\n","class SudokuDataset(Dataset):\n","    \"\"\"\n","    Unified Sudoku dataset combining multiple sources\n","\n","    Args:\n","        process: Processing method for sudoku boards\n","            - \"one-hot\": 729-dim flat vector with one-hot encoding per cell\n","            - \"tokenized\": (81, 9) matrix with 9-dim vector per cell\n","            - \"number\": 81-dim vector with numbers (0 for empty)\n","        base_train_max: Maximum number of train samples from Ritvik19 dataset (default: None = use all)\n","        extreme_train_max: Maximum number of train samples from extreme dataset (default: None = use all)\n","    \"\"\"\n","\n","    def __init__(self, process=\"tokenized\", base_train_max=None, base_test_max=None, extreme_train_max=None, extreme_test_max=None):\n","        super().__init__()\n","\n","        if process not in [\"one-hot\", \"tokenized\", \"number\"]:\n","            raise ValueError(f\"Invalid process type: {process}. Must be 'one-hot', 'tokenized', or 'number'\")\n","\n","        self.process = process\n","        self.base_train_max = base_train_max\n","        self.base_test_max = base_test_max\n","        self.extreme_train_max = extreme_train_max\n","        self.extreme_test_max = extreme_test_max\n","\n","        # Load and process all datasets\n","        self._load_and_process_datasets()\n","\n","        print(f\"\\nDataset loaded successfully!\")\n","        print(f\"Processing method: {process}\")\n","        if base_train_max:\n","            print(f\"Base train max samples: {base_train_max}\")\n","        if base_test_max:\n","            print(f\"Base test max samples: {base_test_max}\")\n","        if extreme_train_max:\n","            print(f\"Extreme train max samples: {extreme_train_max}\")\n","        if extreme_test_max:\n","            print(f\"Extreme test max samples: {extreme_test_max}\")\n","\n","    def _load_and_process_datasets(self):\n","        \"\"\"Load all datasets and process them according to the specified method\"\"\"\n","\n","        # Initialize subspilt storage for train and test\n","        self.train_subsplits = {}\n","        self.test_subsplits = {}\n","\n","        # Load Ritvik19/Sudoku-Dataset\n","        print(\"Loading Ritvik19/Sudoku-Dataset...\")\n","        ritvik_dataset = load_dataset(\"Ritvik19/Sudoku-Dataset\")\n","\n","        # Process Ritvik19 train split into subsplits by missing count\n","        print(\"Processing Ritvik19 train split...\")\n","\n","        # Shuffle and select if base_train_max is specified\n","        ritvik_train = ritvik_dataset['train']\n","        if self.base_train_max:\n","            ritvik_train = ritvik_train.shuffle(seed=42)\n","            ritvik_train = ritvik_train.select(range(min(self.base_train_max, len(ritvik_train))))\n","            print(f\"  Selected {len(ritvik_train)} items from Ritvik19 train (originally {len(ritvik_dataset['train'])})\")\n","\n","        for item in tqdm(ritvik_train, desc=\"Processing Ritvik19 train\"):\n","            missing = item['missing']\n","            if missing not in self.train_subsplits:\n","                self.train_subsplits[missing] = []\n","\n","            question_tensor = self._process_board(item['puzzle'])\n","            answer_tensor = self._process_board(item['solution'])\n","            self.train_subsplits[missing].append((question_tensor, answer_tensor))\n","\n","        # Process Ritvik19 validation split into test subsplits\n","        print(\"Processing Ritvik19 validation split...\")\n","        ritvik_test = ritvik_dataset['validation']\n","        if self.base_test_max:\n","            ritvik_test = ritvik_test.shuffle(seed=42)\n","            ritvik_test = ritvik_test.select(range(min(self.base_test_max, len(ritvik_test))))\n","            print(f\"  Selected {len(ritvik_test)} items from Ritvik19 validation (originally {len(ritvik_dataset['validation'])})\")\n","\n","        for item in tqdm(ritvik_test, desc=\"Processing Ritvik19 validation\", leave=True):\n","            missing = item['missing']\n","            if missing not in self.test_subsplits:\n","                self.test_subsplits[missing] = []\n","\n","            question_tensor = self._process_board(item['puzzle'])\n","            answer_tensor = self._process_board(item['solution'])\n","            self.test_subsplits[missing].append((question_tensor, answer_tensor))\n","\n","        # Load sapientinc/sudoku-extreme\n","        print(\"Loading sapientinc/sudoku-extreme...\")\n","        sudoku_extreme = load_dataset(\"sapientinc/sudoku-extreme\")\n","\n","        # Process extreme train split with optional sampling\n","        print(\"Processing extreme train split...\")\n","\n","        # Shuffle and select if extreme_train_max is specified\n","        extreme_train = sudoku_extreme['train']\n","        if self.extreme_train_max:\n","            extreme_train = extreme_train.shuffle(seed=42)\n","            extreme_train = extreme_train.select(range(min(self.extreme_train_max, len(extreme_train))))\n","            print(f\"  Selected {len(extreme_train)} items from extreme train (originally {len(sudoku_extreme['train'])})\")\n","\n","        self.train_subsplits['extreme'] = self._process_split(\n","            extreme_train['question'],\n","            extreme_train['answer'],\n","            desc=\"Processing extreme train\"\n","        )\n","\n","        # Process extreme test split (always use all data)\n","        print(\"Processing extreme test split...\")\n","        extreme_test = sudoku_extreme['test']\n","\n","        if self.extreme_test_max:\n","            extreme_test = extreme_test.shuffle(seed=42)\n","            extreme_test = extreme_test.select(range(min(self.extreme_test_max, len(extreme_test))))\n","            print(f\"  Selected {len(extreme_test)} items from extreme test (originally {len(sudoku_extreme['test'])})\")\n","\n","        self.test_subsplits['extreme'] = self._process_split(\n","            extreme_test['question'],\n","            extreme_test['answer'],\n","            desc=\"Processing extreme test\"\n","        )\n","\n","        # Load SakanaAI/Sudoku-Bench challenge\n","        print(\"Loading SakanaAI/Sudoku-Bench challenge...\")\n","        sudoku_bench_ch = load_dataset(\"SakanaAI/Sudoku-Bench\", 'challenge_100')\n","        self.challenge = self._process_split(\n","            sudoku_bench_ch['test']['initial_board'],\n","            sudoku_bench_ch['test']['solution'],\n","            desc=\"Processing challenge\"\n","        )\n","\n","        # Load SakanaAI/Sudoku-Bench nikoli\n","        print(\"Loading SakanaAI/Sudoku-Bench nikoli...\")\n","        sudoku_bench_nik = load_dataset(\"SakanaAI/Sudoku-Bench\", 'nikoli_100')\n","        self.nikoli = self._process_split(\n","            sudoku_bench_nik['test']['initial_board'],\n","            sudoku_bench_nik['test']['solution'],\n","            desc=\"Processing nikoli\"\n","        )\n","\n","    def _process_split(self, questions, answers, desc=\"Processing\"):\n","        \"\"\"Process a dataset split according to the specified method\"\"\"\n","        processed_data = []\n","\n","        for q, a in tqdm(zip(questions, answers), total=len(questions), desc=desc, leave=True):\n","            question_tensor = self._process_board(q)\n","            answer_tensor = self._process_board(a)\n","            processed_data.append((question_tensor, answer_tensor))\n","\n","        return processed_data\n","\n","    def _process_board(self, board_str):\n","        \"\"\"Process a single sudoku board string according to the specified method\"\"\"\n","\n","        if self.process == \"number\":\n","            # Convert to 81-dim vector with 0 for empty cells\n","            board_vec = []\n","            for char in board_str:\n","                if char == '.' or char == '0':\n","                    board_vec.append(0)\n","                else:\n","                    board_vec.append(int(char))\n","            return torch.tensor(board_vec, dtype=torch.float32)\n","\n","        elif self.process == \"one-hot\":\n","            # Convert to 729-dim flat one-hot vector\n","            board_vec = []\n","            for char in board_str:\n","                one_hot = [0.0] * 9\n","                if char != '.' and char != '0':\n","                    one_hot[int(char) - 1] = 1.0\n","                board_vec.extend(one_hot)\n","            return torch.tensor(board_vec, dtype=torch.float32)\n","\n","        elif self.process == \"tokenized\":\n","            # Convert to (81, 9) matrix\n","            board_matrix = []\n","            for char in board_str:\n","                token = [0.0] * 9\n","                if char != '.' and char != '0':\n","                    token[int(char) - 1] = 1.0\n","                board_matrix.append(token)\n","            return torch.tensor(board_matrix, dtype=torch.float32)\n","\n","    def get_dataloader(self, split_name, batch_size=32, shuffle=False, min_empty=20, max_empty=64, include_extreme=True):\n","        \"\"\"\n","        Create a DataLoader for a specific split\n","\n","        Args:\n","            split_name: Name of the split ('train', 'test', 'test_extreme', 'challenge', 'nikoli')\n","            batch_size: Batch size for the DataLoader\n","            shuffle: Whether to shuffle the data\n","            min_empty: Minimum number of empty cells for data selection\n","            max_empty: Maximum number of empty cells for data selection\n","            include_extreme: Whether to include extreme data\n","\n","        Returns:\n","            DataLoader for the specified split\n","        \"\"\"\n","        from torch.utils.data import DataLoader\n","\n","        # Get the appropriate data based on split_name\n","        if split_name == 'train':\n","            data = []\n","            # Add data from specified range\n","            for missing in range(min_empty, max_empty + 1):\n","                if missing in self.train_subsplits:\n","                    data.extend(self.train_subsplits[missing])\n","            # Add extreme data if included\n","            if include_extreme and 'extreme' in self.train_subsplits:\n","                data.extend(self.train_subsplits['extreme'])\n","\n","        elif split_name == 'test':\n","            data = []\n","            # Add data from specified range\n","            for missing in range(min_empty, max_empty + 1):\n","                if missing in self.test_subsplits:\n","                    data.extend(self.test_subsplits[missing])\n","            # Add extreme data if included\n","            if include_extreme and 'extreme' in self.test_subsplits:\n","                data.extend(self.test_subsplits['extreme'])\n","\n","        elif split_name == 'test_extreme':\n","            data = self.test_subsplits.get('extreme', [])\n","\n","        elif split_name == 'challenge':\n","            data = self.challenge\n","\n","        elif split_name == 'nikoli':\n","            data = self.nikoli\n","\n","        else:\n","            raise ValueError(f\"Invalid split name: {split_name}\")\n","\n","        class SplitDataset(Dataset):\n","            def __init__(self, data):\n","                self.data = data\n","\n","            def __len__(self):\n","                return len(self.data)\n","\n","            def __getitem__(self, idx):\n","                return self.data[idx]\n","\n","        split_dataset = SplitDataset(data)\n","        return DataLoader(split_dataset, batch_size=batch_size, shuffle=shuffle)\n","\n","    def get_empty_distribution(self, split_name, min_empty=20, max_empty=64):\n","        \"\"\"Get distribution of empty cells for train or test split\"\"\"\n","        if split_name == 'train':\n","            subsplits = self.train_subsplits\n","        elif split_name == 'test':\n","            subsplits = self.test_subsplits\n","        else:\n","            return None\n","\n","        distribution = {}\n","        for key, data in subsplits.items():\n","            if key == 'extreme':\n","                continue  # Skip extreme for distribution\n","            if min_empty <= key <= max_empty:\n","                distribution[key] = len(data)\n","\n","        return distribution\n","\n","def sample_to_grid(tensor, process=\"tokenized\"):\n","    \"\"\"\n","    Convert a processed sudoku tensor back to 9x9 grid format for display\n","\n","    Args:\n","        tensor: Processed sudoku tensor\n","        process: Processing method used (\"one-hot\", \"tokenized\", or \"number\")\n","\n","    Returns:\n","        String representation of the sudoku grid\n","    \"\"\"\n","    # Convert tensor to list of numbers (1-9, 0 for empty)\n","    if process == \"number\":\n","        # Already in number format, just convert 0 to '.'\n","        numbers = tensor.cpu().numpy().astype(int)\n","    elif process == \"one-hot\":\n","        # Reshape from 729 to (81, 9) and get argmax\n","        reshaped = tensor.reshape(81, 9)\n","        numbers = []\n","        for cell in reshaped:\n","            if cell.sum() == 0:\n","                numbers.append(0)\n","            else:\n","                numbers.append(cell.argmax().item() + 1)\n","    elif process == \"tokenized\":\n","        # Shape is (81, 9), get argmax for each cell\n","        numbers = []\n","        for cell in tensor:\n","            if cell.sum() == 0:\n","                numbers.append(0)\n","            else:\n","                numbers.append(cell.argmax().item() + 1)\n","    else:\n","        raise ValueError(f\"Unknown process type: {process}\")\n","\n","    # Build the grid string\n","    grid_str = \"\"\n","    for i in range(9):\n","        if i % 3 == 0 and i != 0:\n","            grid_str += \"------+-------+------\\n\"\n","\n","        for j in range(9):\n","            if j % 3 == 0 and j != 0:\n","                grid_str += \"| \"\n","\n","            idx = i * 9 + j\n","            if numbers[idx] == 0:\n","                grid_str += \". \"\n","            else:\n","                grid_str += f\"{numbers[idx]} \"\n","\n","        grid_str += \"\\n\"\n","\n","    return grid_str\n","\n","def display_sample(dataset, split='train', index=0):\n","    \"\"\"Display a sample from the dataset in grid format\"\"\"\n","    # Note: This function would need to be updated to work with the new structure\n","    pass\n","\n","def save_dataset(dataset, filepath):\n","    \"\"\"Save dataset instance using pickle\"\"\"\n","    with open(filepath, 'wb') as f:\n","        pickle.dump(dataset, f, protocol=pickle.HIGHEST_PROTOCOL)\n","    print(f\"Dataset saved to {filepath}\")\n","\n","def load_saved_dataset(filepath):\n","    \"\"\"Load dataset instance from pickle file\"\"\"\n","    with open(filepath, 'rb') as f:\n","        dataset = pickle.load(f)\n","    print(f\"Dataset loaded from {filepath}\")\n","    return dataset\n","\n","# dataset = SudokuDataset(process=\"tokenized\", base_train_max=100000, extreme_train_max=100000)\n","# save_dataset(dataset, \"/content/drive/MyDrive/Masters Thesis/Idea Experiments/sudoku_tokenized_extended.pkl\")\n","# dataset = load_saved_dataset(\"/content/drive/MyDrive/Masters Thesis/Idea Experiments/sudoku_tokenized_extended.pkl\")"],"metadata":{"id":"NbVQ3d5Y-jrE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import random\n","import numpy as np\n","from tqdm import tqdm\n","\n","\n","def augment_dataset(dataset, augment_factor=32, seed=None):\n","    \"\"\"\n","    Augment training data in-place with geometric transformations and number permutations.\n","\n","    Args:\n","        dataset: SudokuDataset instance to augment\n","        augment_factor: Total number of samples per original (including original)\n","        seed: Random seed for reproducibility\n","\n","    Returns:\n","        The same dataset object with augmented train_subsplits\n","    \"\"\"\n","    if seed is not None:\n","        random.seed(seed)\n","        np.random.seed(seed)\n","        torch.manual_seed(seed)\n","\n","    def rotate_90(tensor):\n","        \"\"\"Rotate 90 degrees clockwise\"\"\"\n","        reshaped = tensor.view(9, 9, -1)\n","        rotated = reshaped.rot90(-1, [0, 1])\n","        return rotated.reshape(81, -1)\n","\n","    def rotate_180(tensor):\n","        \"\"\"Rotate 180 degrees\"\"\"\n","        reshaped = tensor.view(9, 9, -1)\n","        rotated = reshaped.rot90(-2, [0, 1])\n","        return rotated.reshape(81, -1)\n","\n","    def rotate_270(tensor):\n","        \"\"\"Rotate 270 degrees clockwise\"\"\"\n","        reshaped = tensor.view(9, 9, -1)\n","        rotated = reshaped.rot90(-3, [0, 1])\n","        return rotated.reshape(81, -1)\n","\n","    def flip_vertical(tensor):\n","        \"\"\"Flip vertically\"\"\"\n","        reshaped = tensor.view(9, 9, -1)\n","        flipped = reshaped.flip(0)\n","        return flipped.reshape(81, -1)\n","\n","    def flip_horizontal(tensor):\n","        \"\"\"Flip horizontally\"\"\"\n","        reshaped = tensor.view(9, 9, -1)\n","        flipped = reshaped.flip(1)\n","        return flipped.reshape(81, -1)\n","\n","    def transpose(tensor):\n","        \"\"\"Transpose (flip along main diagonal)\"\"\"\n","        reshaped = tensor.view(9, 9, -1)\n","        transposed = reshaped.transpose(0, 1)\n","        return transposed.reshape(81, -1)\n","\n","    def anti_transpose(tensor):\n","        \"\"\"Anti-transpose (flip along anti-diagonal)\"\"\"\n","        reshaped = tensor.view(9, 9, -1)\n","        anti_transposed = reshaped.rot90(-1, [0, 1]).transpose(0, 1)\n","        return anti_transposed.reshape(81, -1)\n","\n","    def apply_number_permutation(input_tensor, target_tensor):\n","        \"\"\"Apply random number permutation\"\"\"\n","        perm = torch.randperm(9)\n","        return input_tensor[:, perm], target_tensor[:, perm]\n","\n","    # Geometric transformations\n","    geometric_transforms = [\n","        lambda x: x,  # Identity\n","        rotate_90,\n","        rotate_180,\n","        rotate_270,\n","        flip_vertical,\n","        flip_horizontal,\n","        transpose,\n","        anti_transpose,\n","    ]\n","\n","    print(f\"Augmenting training data with factor {augment_factor}...\")\n","\n","    # Count total samples\n","    total_samples = sum(len(data) for data in dataset.train_subsplits.values())\n","\n","    with tqdm(total=total_samples, desc=\"Augmenting data\", unit=\"samples\") as pbar:\n","        # Augment each subsplit\n","        for key in dataset.train_subsplits:\n","            original_data = dataset.train_subsplits[key]\n","            augmented_data = []\n","\n","            pbar.set_description(f\"Augmenting subsplit '{key}'\")\n","\n","            for input_t, target_t in original_data:\n","                # Add original\n","                augmented_data.append((input_t, target_t))\n","\n","                # Add augmented versions\n","                for _ in range(augment_factor - 1):\n","                    # Random geometric transformation\n","                    geo_idx = random.randint(0, 7)\n","                    aug_input = geometric_transforms[geo_idx](input_t.clone())\n","                    aug_target = geometric_transforms[geo_idx](target_t.clone())\n","\n","                    # 50% chance for number permutation\n","                    if random.random() < 0.5:\n","                        aug_input, aug_target = apply_number_permutation(aug_input, aug_target)\n","\n","                    augmented_data.append((aug_input, aug_target))\n","\n","                pbar.update(1)\n","\n","            # Replace with augmented data\n","            dataset.train_subsplits[key] = augmented_data\n","            tqdm.write(f\"  Subsplit '{key}': {len(original_data)} -> {len(augmented_data)} samples\")\n","\n","    print(f\"Augmentation complete! Total samples: {sum(len(data) for data in dataset.train_subsplits.values())}\")\n","\n","    return dataset"],"metadata":{"id":"YKXA96RBSpHY"},"execution_count":null,"outputs":[]}]}