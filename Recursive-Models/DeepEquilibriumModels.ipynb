{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyPFw5HPtHQfIsWrelV4dTGu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","\n","\n","class StaticDEQ(nn.Module):\n","    \"\"\"\n","    Static Deep Equilibrium Model\n","\n","    Args:\n","        T: Sequence length\n","        C: Input/output channel dimension\n","        D: Hidden dimension\n","        L: Number of iterations\n","        N: Number of weight matrices in update function\n","        hid_activation: Hidden activation function ('sigmoid', 'token_softmax', 'relu', 'gelu', 'tanh')\n","        output_activation: Output activation function ('sigmoid', 'token_softmax', 'relu', 'gelu', 'tanh', 'none')\n","        weight_init: Weight initialization method ('xavier_uniform', 'xavier_normal',\n","                                                   'kaiming_uniform', 'kaiming_normal', 'orthogonal')\n","        bias: Whether to use bias in linear projections (default: False)\n","    \"\"\"\n","\n","    def __init__(self, T, C, D, L, N, hid_activation='relu', output_activation='none',\n","                 weight_init='xavier_uniform', bias=False):\n","        super(StaticDEQ, self).__init__()\n","\n","        self.T = T\n","        self.C = C\n","        self.D = D\n","        self.L = L\n","        self.N = N\n","        self.hid_activation = hid_activation\n","        self.output_activation = output_activation\n","        self.use_bias = bias\n","\n","        # Input and output projections\n","        self.input_proj = nn.Linear(C, D, bias=bias)\n","        self.output_proj = nn.Linear(D, C, bias=bias)\n","\n","        # N weight tensors of shape (T, T, D, D)\n","        self.weights = nn.ParameterList([\n","            nn.Parameter(torch.empty(T, T, D, D))\n","            for _ in range(N)\n","        ])\n","\n","        # Initialize weights\n","        self._initialize_weights(weight_init)\n","\n","    def _initialize_weights(self, method):\n","        \"\"\"Initialize all weights\"\"\"\n","        # Initialize weight tensors\n","        for weight in self.weights:\n","            if method == 'xavier_uniform':\n","                nn.init.xavier_uniform_(weight.view(-1, self.D))\n","            elif method == 'xavier_normal':\n","                nn.init.xavier_normal_(weight.view(-1, self.D))\n","            elif method == 'kaiming_uniform':\n","                nn.init.kaiming_uniform_(weight.view(-1, self.D), nonlinearity='relu')\n","            elif method == 'kaiming_normal':\n","                nn.init.kaiming_normal_(weight.view(-1, self.D), nonlinearity='relu')\n","            elif method == 'orthogonal':\n","                for i in range(self.T):\n","                    for j in range(self.T):\n","                        nn.init.orthogonal_(weight[i, j])\n","            else:\n","                raise ValueError(f\"Unknown initialization method: {method}\")\n","\n","        # Initialize projection layers\n","        if method in ['xavier_uniform', 'xavier_normal']:\n","            if method == 'xavier_uniform':\n","                nn.init.xavier_uniform_(self.input_proj.weight)\n","                nn.init.xavier_uniform_(self.output_proj.weight)\n","            else:\n","                nn.init.xavier_normal_(self.input_proj.weight)\n","                nn.init.xavier_normal_(self.output_proj.weight)\n","        elif method in ['kaiming_uniform', 'kaiming_normal']:\n","            if method == 'kaiming_uniform':\n","                nn.init.kaiming_uniform_(self.input_proj.weight, nonlinearity='relu')\n","                nn.init.kaiming_uniform_(self.output_proj.weight, nonlinearity='relu')\n","            else:\n","                nn.init.kaiming_normal_(self.input_proj.weight, nonlinearity='relu')\n","                nn.init.kaiming_normal_(self.output_proj.weight, nonlinearity='relu')\n","        elif method == 'orthogonal':\n","            nn.init.orthogonal_(self.input_proj.weight)\n","            nn.init.orthogonal_(self.output_proj.weight)\n","\n","        # Initialize biases to zero if they exist\n","        if self.use_bias:\n","            nn.init.zeros_(self.input_proj.bias)\n","            nn.init.zeros_(self.output_proj.bias)\n","\n","    def _get_activation(self, activation_name):\n","        \"\"\"Get activation function by name\"\"\"\n","        if activation_name == 'sigmoid':\n","            return torch.sigmoid\n","        elif activation_name == 'token_softmax':\n","            return lambda x: F.softmax(x, dim=-1)\n","        elif activation_name == 'relu':\n","            return F.relu\n","        elif activation_name == 'gelu':\n","            return F.gelu\n","        elif activation_name == 'tanh':\n","            return torch.tanh\n","        elif activation_name == 'none':\n","            return lambda x: x\n","        else:\n","            raise ValueError(f\"Unknown activation: {activation_name}\")\n","\n","    def update_function(self, X):\n","        \"\"\"\n","        Apply the sequential update function\n","        Args:\n","            X: Input tensor of shape (B, T, D)\n","        Returns:\n","            Updated tensor of shape (B, T, D)\n","        \"\"\"\n","        hid_act = self._get_activation(self.hid_activation)\n","\n","        for n in range(self.N):\n","            # Apply weight_n: einsum('ttdd,btd->btd', weight_n, X)\n","            X_ = torch.einsum('ttdd,btd->btd', self.weights[n], X)\n","            # Apply activation\n","            X = X + hid_act(X_)\n","\n","        return X\n","\n","    def forward(self, X):\n","        \"\"\"\n","        Forward pass through the Static DEQ model\n","        Args:\n","            X: Input tensor of shape (B, T, C)\n","        Returns:\n","            Output tensor of shape (B, T, C)\n","        \"\"\"\n","        B, T, C = X.shape\n","        assert T == self.T, f\"Input sequence length {T} doesn't match model's T={self.T}\"\n","        assert C == self.C, f\"Input channel dimension {C} doesn't match model's C={self.C}\"\n","\n","        # Input projection: (B, T, C) -> (B, T, D)\n","        X = self.input_proj(X)\n","\n","        # Iterative updates\n","        for l in range(self.L):\n","            X = X + self.update_function(X)\n","\n","        # Output projection: (B, T, D) -> (B, T, C)\n","        X = self.output_proj(X)\n","\n","        # Output activation\n","        output_act = self._get_activation(self.output_activation)\n","        X = output_act(X)\n","\n","        return X"],"metadata":{"id":"EJVsTXszZQoM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class HierarchicalDEQ(nn.Module):\n","    \"\"\"\n","    Hierarchical Deep Equilibrium Model\n","\n","    Args:\n","        C: Input/output channel dimension\n","        D: Hidden dimension\n","        Ls: List of iteration counts for each stage [l_0, ..., l_S]\n","        Ns: List of weight matrix counts for each stage [n_0, ..., n_S]\n","        s_dims: List of dimensions [d_0=T, d_1, ..., d_S]\n","        hid_activation: Hidden activation function ('sigmoid', 'token_softmax', 'relu', 'gelu', 'tanh')\n","        output_activation: Output activation function ('sigmoid', 'token_softmax', 'relu', 'gelu', 'tanh', 'none')\n","        weight_init: Weight initialization method ('xavier_uniform', 'xavier_normal',\n","                                                   'kaiming_uniform', 'kaiming_normal', 'orthogonal')\n","        bias: Whether to use bias in linear projections (default: False)\n","        weight_share: Whether to share weights across stages (default: False)\n","    \"\"\"\n","\n","    def __init__(self, C, D, Ls, Ns, s_dims, hid_activation='relu', output_activation='none',\n","                 weight_init='xavier_uniform', bias=False, weight_share=False):\n","        super(HierarchicalDEQ, self).__init__()\n","\n","        self.C = C\n","        self.D = D\n","        self.Ls = Ls\n","        self.Ns = Ns\n","        self.s_dims = s_dims\n","        self.T = s_dims[0]  # d_0 = T\n","        self.S = len(Ls) - 1  # Number of stages (0 to S)\n","        self.hid_activation = hid_activation\n","        self.output_activation = output_activation\n","        self.use_bias = bias\n","        self.weight_share = weight_share\n","\n","        # Total dimension for concatenated XZ\n","        self.total_dim = sum(s_dims)\n","\n","        # Input and output projections\n","        self.input_proj = nn.Linear(C, D, bias=bias)\n","        self.output_proj = nn.Linear(D, C, bias=bias)\n","\n","        # Learnable latent vectors Z_s for s = 1, ..., S (Z_0 = X, not learnable)\n","        self.Z = nn.ParameterList([\n","            nn.Parameter(torch.empty(s_dims[s], D)) for s in range(1, len(s_dims))\n","        ])\n","\n","        # Weight matrices for update functions\n","        if weight_share:\n","            # Use n_0 for all stages when weight sharing\n","            N = Ns[0]\n","            self.weights = nn.ParameterList([\n","                nn.Parameter(torch.empty(self.total_dim, self.total_dim, D, D))\n","                for _ in range(N)\n","            ])\n","        else:\n","            # Separate weights for each stage\n","            self.weights = nn.ModuleList([\n","                nn.ParameterList([\n","                    nn.Parameter(torch.empty(self.total_dim, self.total_dim, D, D))\n","                    for _ in range(Ns[s])\n","                ])\n","                for s in range(self.S + 1)\n","            ])\n","\n","        # Initialize weights\n","        self._initialize_weights(weight_init)\n","\n","    def _initialize_weights(self, method):\n","        \"\"\"Initialize all weights\"\"\"\n","        # Initialize latent vectors\n","        for z in self.Z:\n","            if method == 'xavier_uniform':\n","                nn.init.xavier_uniform_(z)\n","            elif method == 'xavier_normal':\n","                nn.init.xavier_normal_(z)\n","            elif method == 'kaiming_uniform':\n","                nn.init.kaiming_uniform_(z, nonlinearity='relu')\n","            elif method == 'kaiming_normal':\n","                nn.init.kaiming_normal_(z, nonlinearity='relu')\n","            elif method == 'orthogonal':\n","                nn.init.orthogonal_(z)\n","            else:\n","                raise ValueError(f\"Unknown initialization method: {method}\")\n","\n","        # Initialize weight tensors\n","        if self.weight_share:\n","            weights_to_init = self.weights\n","        else:\n","            weights_to_init = []\n","            for stage_weights in self.weights:\n","                weights_to_init.extend(stage_weights)\n","\n","        for weight in weights_to_init:\n","            if method == 'xavier_uniform':\n","                nn.init.xavier_uniform_(weight.view(-1, self.D))\n","            elif method == 'xavier_normal':\n","                nn.init.xavier_normal_(weight.view(-1, self.D))\n","            elif method == 'kaiming_uniform':\n","                nn.init.kaiming_uniform_(weight.view(-1, self.D), nonlinearity='relu')\n","            elif method == 'kaiming_normal':\n","                nn.init.kaiming_normal_(weight.view(-1, self.D), nonlinearity='relu')\n","            elif method == 'orthogonal':\n","                for i in range(self.total_dim):\n","                    for j in range(self.total_dim):\n","                        nn.init.orthogonal_(weight[i, j])\n","            else:\n","                raise ValueError(f\"Unknown initialization method: {method}\")\n","\n","        # Initialize projection layers\n","        if method in ['xavier_uniform', 'xavier_normal']:\n","            if method == 'xavier_uniform':\n","                nn.init.xavier_uniform_(self.input_proj.weight)\n","                nn.init.xavier_uniform_(self.output_proj.weight)\n","            else:\n","                nn.init.xavier_normal_(self.input_proj.weight)\n","                nn.init.xavier_normal_(self.output_proj.weight)\n","        elif method in ['kaiming_uniform', 'kaiming_normal']:\n","            if method == 'kaiming_uniform':\n","                nn.init.kaiming_uniform_(self.input_proj.weight, nonlinearity='relu')\n","                nn.init.kaiming_uniform_(self.output_proj.weight, nonlinearity='relu')\n","            else:\n","                nn.init.kaiming_normal_(self.input_proj.weight, nonlinearity='relu')\n","                nn.init.kaiming_normal_(self.output_proj.weight, nonlinearity='relu')\n","        elif method == 'orthogonal':\n","            nn.init.orthogonal_(self.input_proj.weight)\n","            nn.init.orthogonal_(self.output_proj.weight)\n","\n","        # Initialize biases to zero if they exist\n","        if self.use_bias:\n","            nn.init.zeros_(self.input_proj.bias)\n","            nn.init.zeros_(self.output_proj.bias)\n","\n","    def _get_activation(self, activation_name):\n","        \"\"\"Get activation function by name\"\"\"\n","        if activation_name == 'sigmoid':\n","            return torch.sigmoid\n","        elif activation_name == 'token_softmax':\n","            return lambda x: F.softmax(x, dim=-1)\n","        elif activation_name == 'relu':\n","            return F.relu\n","        elif activation_name == 'gelu':\n","            return F.gelu\n","        elif activation_name == 'tanh':\n","            return torch.tanh\n","        elif activation_name == 'none':\n","            return lambda x: x\n","        else:\n","            raise ValueError(f\"Unknown activation: {activation_name}\")\n","\n","    def update_function(self, XZ, stage):\n","        \"\"\"\n","        Apply the update function for a specific stage, updating only the corresponding part\n","        Args:\n","            XZ: Concatenated tensor of shape (B, sum(s_dims), D)\n","            stage: Stage index (0 to S)\n","        Returns:\n","            Updated tensor of shape (B, sum(s_dims), D) with only stage part updated\n","        \"\"\"\n","        hid_act = self._get_activation(self.hid_activation)\n","\n","        if self.weight_share:\n","            # Use shared weights (n_0 weights)\n","            weights = self.weights\n","            N = self.Ns[0]\n","        else:\n","            # Use stage-specific weights\n","            weights = self.weights[stage]\n","            N = self.Ns[stage]\n","\n","        # Determine which part to update\n","        if stage == 0:\n","            start_idx = 0\n","            end_idx = self.T\n","        else:\n","            start_idx = sum(self.s_dims[:stage])\n","            end_idx = start_idx + self.s_dims[stage]\n","\n","        # Keep original input for parts we don't update\n","        XZ_original = XZ\n","        # Part to update\n","        XZ_part = XZ[:, start_idx:end_idx, :]\n","\n","        for n in range(N):\n","            # Apply weight to entire concatenated vector\n","            XZ_transformed = torch.einsum('ttdd,btd->btd', weights[n], XZ)\n","            # Apply activation\n","            XZ_transformed = hid_act(XZ_transformed)\n","            # Extract only the part we want to update\n","            XZ_part_ = XZ_transformed[:, start_idx:end_idx, :]\n","            # Reconstruct XZ with updated part\n","            if start_idx == 0:\n","                if end_idx < XZ.shape[1]:\n","                    XZ_part = XZ_part + XZ_part_\n","                    XZ = torch.cat([XZ_part, XZ_original[:, end_idx:, :]], dim=1)\n","                else:\n","                    XZ = XZ + XZ_part\n","            else:\n","                if end_idx < XZ.shape[1]:\n","                    XZ_part = XZ_part + XZ_part_\n","                    XZ = torch.cat([XZ_original[:, :start_idx, :], XZ_part, XZ_original[:, end_idx:, :]], dim=1)\n","                else:\n","                    XZ_part = XZ_part + XZ_part_\n","                    XZ = torch.cat([XZ_original[:, :start_idx, :], XZ_part], dim=1)\n","\n","        return XZ\n","\n","    def run_stage(self, X, Z_all, stage):\n","        \"\"\"\n","        Run iterations for a specific stage, updating only the corresponding vector\n","        Args:\n","            X: Current X tensor of shape (B, T, D)\n","            Z_all: List of all Z tensors [Z_1, ..., Z_S] with shape (B, d_s, D)\n","            stage: Current stage (0 to S)\n","        Returns:\n","            Updated X or Z_all depending on stage\n","        \"\"\"\n","        B = X.shape[0]\n","\n","        for l in range(self.Ls[stage]):\n","            # Concatenate X and all Z_s\n","            XZ = torch.cat([X] + Z_all, dim=1)\n","\n","            # Apply update function\n","            XZ_new = self.update_function(XZ, stage)\n","\n","            # Extract and update only the part corresponding to this stage\n","            if stage == 0:\n","                # Update X only (first T dimensions)\n","                X = XZ_new[:, :self.T, :]\n","            else:\n","                # Update Z_stage only\n","                start_idx = sum(self.s_dims[:stage])\n","                end_idx = start_idx + self.s_dims[stage]\n","                Z_all[stage - 1] = XZ_new[:, start_idx:end_idx, :]\n","\n","            # Recursive call for inner stages\n","            if stage > 0:\n","                X, Z_all = self.run_stage(X, Z_all, stage - 1)\n","\n","        return X, Z_all\n","\n","    def forward(self, X):\n","        \"\"\"\n","        Forward pass through the Hierarchical DEQ model\n","        Args:\n","            X: Input tensor of shape (B, T, C)\n","        Returns:\n","            Output tensor of shape (B, T, C)\n","        \"\"\"\n","        B, T, C = X.shape\n","        assert T == self.T, f\"Input sequence length {T} doesn't match model's T={self.T}\"\n","        assert C == self.C, f\"Input channel dimension {C} doesn't match model's C={self.C}\"\n","\n","        # Input projection: (B, T, C) -> (B, T, D)\n","        X = self.input_proj(X)\n","\n","        # Expand latent vectors to batch dimension\n","        Z_all = [z.unsqueeze(0).expand(B, -1, -1) for z in self.Z]\n","\n","        # Start hierarchical forward pass from outermost stage S\n","        X, _ = self.run_stage(X, Z_all, self.S)\n","\n","        # Output projection: (B, T, D) -> (B, T, C)\n","        X = self.output_proj(X)\n","\n","        # Output activation\n","        output_act = self._get_activation(self.output_activation)\n","        X = output_act(X)\n","\n","        return X"],"metadata":{"id":"6dnmE0UjoIQb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","\n","\n","class HyperDEQ(nn.Module):\n","    \"\"\"\n","    Hyper Deep Equilibrium Model\n","\n","    Args:\n","        T: Sequence length\n","        C: Input/output channel dimension\n","        D: Hidden dimension\n","        L: Number of iterations\n","        N: Number of weight generation steps\n","        H: Number of heads\n","        E: Head dimension\n","        hid_activation: Hidden activation function ('sigmoid', 'token_softmax', 'relu', 'gelu', 'tanh')\n","        output_activation: Output activation function ('sigmoid', 'token_softmax', 'relu', 'gelu', 'tanh', 'none')\n","        weight_init: Weight initialization method ('xavier_uniform', 'xavier_normal',\n","                                                   'kaiming_uniform', 'kaiming_normal', 'orthogonal')\n","        bias: Whether to use bias in linear projections (default: False)\n","        weight_share: Whether to share Wq, Wk, Wv, Wo across n (default: False)\n","    \"\"\"\n","\n","    def __init__(self, T, C, D, L, N, H, E, hid_activation='relu', output_activation='none',\n","                 weight_init='xavier_uniform', bias=False, weight_share=False):\n","        super(HyperDEQ, self).__init__()\n","\n","        self.T = T\n","        self.C = C\n","        self.D = D\n","        self.L = L\n","        self.N = N\n","        self.H = H\n","        self.E = E\n","        self.hid_activation = hid_activation\n","        self.output_activation = output_activation\n","        self.use_bias = bias\n","        self.weight_share = weight_share\n","\n","        # Input and output projections\n","        self.input_proj = nn.Linear(C, D, bias=bias)\n","        self.output_proj = nn.Linear(D, C, bias=bias)\n","\n","        # Weight generation parameters\n","        if weight_share:\n","            # Shared across all n\n","            self.Wq = nn.Parameter(torch.empty(H, D, E))\n","            self.Wk = nn.Parameter(torch.empty(H, D, E))\n","            self.Wv = nn.Parameter(torch.empty(H, D, E))\n","            self.Wo = nn.Parameter(torch.empty(H, E, D))\n","        else:\n","            # Separate for each n\n","            self.Wq = nn.ParameterList([\n","                nn.Parameter(torch.empty(H, D, E)) for _ in range(N)\n","            ])\n","            self.Wk = nn.ParameterList([\n","                nn.Parameter(torch.empty(H, D, E)) for _ in range(N)\n","            ])\n","            self.Wv = nn.ParameterList([\n","                nn.Parameter(torch.empty(H, D, E)) for _ in range(N)\n","            ])\n","            self.Wo = nn.ParameterList([\n","                nn.Parameter(torch.empty(H, E, D)) for _ in range(N)\n","            ])\n","\n","        # Initialize weights\n","        self._initialize_weights(weight_init)\n","\n","    def _initialize_weights(self, method):\n","        \"\"\"Initialize all weights\"\"\"\n","        # Initialize weight generation parameters\n","        if self.weight_share:\n","            params_to_init = [self.Wq, self.Wk, self.Wv, self.Wo]\n","        else:\n","            params_to_init = []\n","            params_to_init.extend(self.Wq)\n","            params_to_init.extend(self.Wk)\n","            params_to_init.extend(self.Wv)\n","            params_to_init.extend(self.Wo)\n","\n","        for param in params_to_init:\n","            # Reshape for initialization\n","            shape = param.shape\n","            if len(shape) == 3:\n","                # For Wq, Wk, Wv, Wo: (H, D, E) or (H, E, D)\n","                param_2d = param.view(-1, shape[-1])\n","            else:\n","                param_2d = param\n","\n","            if method == 'xavier_uniform':\n","                nn.init.xavier_uniform_(param_2d)\n","            elif method == 'xavier_normal':\n","                nn.init.xavier_normal_(param_2d)\n","            elif method == 'kaiming_uniform':\n","                nn.init.kaiming_uniform_(param_2d, nonlinearity='relu')\n","            elif method == 'kaiming_normal':\n","                nn.init.kaiming_normal_(param_2d, nonlinearity='relu')\n","            elif method == 'orthogonal':\n","                nn.init.orthogonal_(param_2d)\n","            else:\n","                raise ValueError(f\"Unknown initialization method: {method}\")\n","\n","            # Reshape back\n","            if len(shape) == 3:\n","                param.data = param_2d.view(shape)\n","\n","        # Initialize projection layers\n","        if method in ['xavier_uniform', 'xavier_normal']:\n","            if method == 'xavier_uniform':\n","                nn.init.xavier_uniform_(self.input_proj.weight)\n","                nn.init.xavier_uniform_(self.output_proj.weight)\n","            else:\n","                nn.init.xavier_normal_(self.input_proj.weight)\n","                nn.init.xavier_normal_(self.output_proj.weight)\n","        elif method in ['kaiming_uniform', 'kaiming_normal']:\n","            if method == 'kaiming_uniform':\n","                nn.init.kaiming_uniform_(self.input_proj.weight, nonlinearity='relu')\n","                nn.init.kaiming_uniform_(self.output_proj.weight, nonlinearity='relu')\n","            else:\n","                nn.init.kaiming_normal_(self.input_proj.weight, nonlinearity='relu')\n","                nn.init.kaiming_normal_(self.output_proj.weight, nonlinearity='relu')\n","        elif method == 'orthogonal':\n","            nn.init.orthogonal_(self.input_proj.weight)\n","            nn.init.orthogonal_(self.output_proj.weight)\n","\n","        # Initialize biases to zero if they exist\n","        if self.use_bias:\n","            nn.init.zeros_(self.input_proj.bias)\n","            nn.init.zeros_(self.output_proj.bias)\n","\n","    def _get_activation(self, activation_name):\n","        \"\"\"Get activation function by name\"\"\"\n","        if activation_name == 'sigmoid':\n","            return torch.sigmoid\n","        elif activation_name == 'token_softmax':\n","            return lambda x: F.softmax(x, dim=-1)\n","        elif activation_name == 'relu':\n","            return F.relu\n","        elif activation_name == 'gelu':\n","            return F.gelu\n","        elif activation_name == 'tanh':\n","            return torch.tanh\n","        elif activation_name == 'none':\n","            return lambda x: x\n","        else:\n","            raise ValueError(f\"Unknown activation: {activation_name}\")\n","\n","    def update_weight_n(self, X_exp, n):\n","        \"\"\"\n","        Generate weight_n using attention mechanism\n","        Args:\n","            X_exp: Input tensor of shape (B, H, T, D)\n","            n: Index for weight generation parameters\n","        Returns:\n","            weight_n: Generated weight tensor of shape (B, H, T, T, D, D)\n","        \"\"\"\n","        B, H, T, D = X_exp.shape\n","\n","        # Get parameters for this n\n","        if self.weight_share:\n","            Wq = self.Wq\n","            Wk = self.Wk\n","            Wv = self.Wv\n","            Wo = self.Wo\n","        else:\n","            Wq = self.Wq[n]\n","            Wk = self.Wk[n]\n","            Wv = self.Wv[n]\n","            Wo = self.Wo[n]\n","\n","        # Q: (B, H, T, E) = X_exp @ Wq\n","        Q = torch.einsum('bhtd,hde->bhte', X_exp, Wq)\n","\n","        # K: (B, H, T, E) = X_exp @ Wk\n","        K = torch.einsum('bhtd,hde->bhte', X_exp, Wk)\n","\n","        # K: (B, H, E, T) = transpose\n","        K = K.transpose(-2, -1)\n","\n","        # A: (B, H, T, T) = softmax(Q @ K / sqrt(E), dim=-1)\n","        A = F.softmax(Q @ K / math.sqrt(self.E), dim=-1)\n","\n","        # weight_n: (B, H, T, T, D, D) = einsum(\"bhti,hde,hej->bhtidj\", A, Wv, Wo)\n","        weight_n = torch.einsum('bhti,hde,hej->bhtidj', A, Wv, Wo)\n","\n","        return weight_n\n","\n","    def update_function(self, X_exp, weight_n):\n","        \"\"\"\n","        Apply the update function\n","        Args:\n","            X_exp: Input tensor of shape (B, H, T, D)\n","            weight_n: Weight tensor of shape (B, H, T, T, D, D)\n","        Returns:\n","            Updated tensor of shape (B, H, T, D)\n","        \"\"\"\n","        hid_act = self._get_activation(self.hid_activation)\n","\n","        # Apply weight_n: einsum('bhttdd,bhtd->bhtd', weight_n, X_exp)\n","        X_exp_ = torch.einsum('bhttdd,bhtd->bhtd', weight_n, X_exp)\n","\n","        # Apply activation\n","        X_exp_ = hid_act(X_exp_)\n","\n","        return X_exp_\n","\n","    def forward(self, X):\n","        \"\"\"\n","        Forward pass through the Hyper DEQ model\n","        Args:\n","            X: Input tensor of shape (B, T, C)\n","        Returns:\n","            Output tensor of shape (B, T, C)\n","        \"\"\"\n","        B, T, C = X.shape\n","        assert T == self.T, f\"Input sequence length {T} doesn't match model's T={self.T}\"\n","        assert C == self.C, f\"Input channel dimension {C} doesn't match model's C={self.C}\"\n","\n","        # Input projection: (B, T, C) -> (B, T, D)\n","        X = self.input_proj(X)\n","\n","        # Give H dimension to X: (B, T, D) -> (B, H, T, D)\n","        X_exp = X.unsqueeze(1).expand(-1, self.H, -1, -1)\n","\n","        # Main loop\n","        for n in range(self.N):\n","            # Generate weight_n\n","            weight_n = self.update_weight_n(X_exp, n)\n","\n","            # Apply L iterations\n","            for l in range(self.L):\n","                X_exp = X_exp + self.update_function(X_exp, weight_n)\n","\n","        # Average over H dimension: (B, H, T, D) -> (B, T, D)\n","        X = X_exp.mean(dim=1)\n","\n","        # Output projection: (B, T, D) -> (B, T, C)\n","        X = self.output_proj(X)\n","\n","        # Output activation\n","        output_act = self._get_activation(self.output_activation)\n","        X = output_act(X)\n","\n","        return X"],"metadata":{"id":"w3nVI1j16hq7"},"execution_count":null,"outputs":[]}]}